\documentclass{kththesis}

\usepackage{blindtext} % This is just to get some nonsense text in this template, can be safely removed

\usepackage{minted}

\usepackage{syntax}

\usepackage{csquotes} % Recommended by biblatex
\usepackage{biblatex}
\addbibresource{references.bib} % The file containing our references, in BibTeX format

\title{Compositional Programming Language Design}
\alttitle{Detta är den svenska översättningen av titeln}
\author{Viktor Palmkvist}
\email{vipa@kth.se}
\supervisor{David Broman}
\examiner{Mads Dam}
\programme{Master in Computer Science}
\school{School of Computer Science and Communication}
\date{\today}


\begin{document}

% Frontmatter includes the titlepage, abstracts and table-of-contents
\frontmatter

\titlepage

\begin{abstract}
TBW
\end{abstract}


\begin{otherlanguage}{swedish}
\begin{abstract}
Kommer så småningom
\end{abstract}
\end{otherlanguage}


\tableofcontents


% Mainmatter is where the actual contents of the thesis goes
\mainmatter


\chapter{Introduction} \label{sec:introduction}

The implementation of a programming language---be it an interpreter or a compiler---tends to be divided into phases, each dealing with their own aspect of the language. For example, a parser details the syntax of the language, while name resolution handles names, scoping, and namespaces. A type checker details part of the semantics, how different language constructions can be combined. This means that the implementation of any single language construction is spread throughout the language implementation, necessitating it be considered as a whole. Adding a new construction requires changes in many places.

Some languages include methods to add new constructions within the language itself, without extending the implementation; one important one being macros \cite{plt-tr1,Hickey2008,Matsakis2014}. Macros add new constructions by defining them in terms of previously existing constructions. Evaluation proceeds by expanding each macro, possibly recursively, until only base language constructions remain, at which point the program can be evaluated normally.

When looking at macros in languages that are commonly used today a few problems remain however. This thesis will focus on two broad categories of such problems: abstraction (see section \ref{sec:problem-abstraction}) and syntax (see section \ref{sec:problem-syntax}). These problems have been solved separately (see sections \ref{sec:abstraction-solutions} and \ref{sec:syntax-solutions}) as well as in conjunction (see section \ref{sec:full-solutions}). This thesis falls into the second camp and largely addresses both problems, with a new set of trade-offs and a different unit of composition. See section \ref{sec:solution-overview} for an overview and section \ref{sec:thesis-solution} for more details.

\section{Problems of Abstraction} \label{sec:problem-abstraction}

The abstractions provided by macros can easily become leaky abstractions without special care.

\subsection{Name Capture}

A naive macro expander using textual replacement will introduce accidental name capture. For example, given the following macro (using Racket \cite{plt-tr1} syntax) intended to calculate $2a + b$ while computing $a$ only once:

\begin{minted}{racket}
(define-syntax-rule (double-add a b)
  (let ([x a])
    (+ x x b)))
\end{minted}

The following expression will exhibit accidental name capture when macro expansion uses pure textual replacement:

\begin{minted}{racket}
(let ([x 2])
  (double-add 1 x))
; expands to:
(let ([x 2])
  (let ([x 1])
    (double-add x x x)))
; which evaluates to
3
; while our intended semantics would evaluate to:
4
\end{minted}

Notable here is that the above would not produce an error of any kind, the result would simply be silently wrong.

Most macro systems will avoid this problem through various forms of rewriting or more complicated forms of name resolution (e.g. \cite{Flatt:2016:BSS:2837614.2837620} TODO: references), a notable exception being the C preprocessor that makes no such attempt.

\subsection{Errors after Expansion}

In some cases the fully expanded code is incorrect, either because the macro was poorly implemented, or because it was used improperly. For example:

\begin{minted}{racket}
(define-syntax-rule (new-let x e body)
  (let ([x e])
    body))

(new-let 2 x
  (+ 1 x))
\end{minted}

Here we define a new binding construction, but misuse it; the identifier and the value are switched. The macro expansion will succeed and produce the code below, but at that point an error message will be produced about improper use of \mintinline{racket}{let}, even though \mintinline{racket}{let} never occurs in the original code.

\begin{minted}{racket}
(let ([2 x])
  (+ 1 x))
\end{minted}

The error exposes the implementation of the macro. The generated code may be introduced through several layers of macro expansion and can be quite complicated, making the connection between the error message and the actual error even less clear.

In a statically typed language this problem has an additional form: type errors after macro expansion.

\section{Problems of Syntax} \label{sec:problem-syntax}

One advantage of macros is their ability to essentially introduce syntactic sugar to a language at a user level. Despite this most commonly used macro systems place some fairly strict limits on the newly introduced syntax. For example, a C preprocessor macro must be an identifier, possibly followed by an argument list, and a lisp macro must be a list of valid forms started by a symbol.

Such restrictions simplify parsing the language, since it ensures the grammar is fixed and cannot be changed during parsing.

\section{Contributions}

This thesis contributes the following:

\begin{itemize}
  \item A method of specifying the syntax, binding semantics and implementation of a language construction (macro) in a singular location (sections \ref{sec:design-syntax}, \ref{sec:design-bindings} and \ref{sec:design-implementation}).
  \item Proof that macro expansion using the above method terminates, given a finite input (section \ref{sec:proof-termination}).
  \item Proof that macro expansion does not introduce any binding errors (alternatively preservation of $\alpha$-equivalence) (section \ref{sec:proof-no-errors}).
  \item An implementation of a non-trivial subset of JavaScript/Lua (section \ref{sec:imperative-eval}) to evaluate the method's suitability in modeling a typical imperative language.
  \item An implementation of a non-trivial subset of OCaml (section \ref{sec:functional-eval}) to evaluate the method's suitability in modeling a typical functional language, focusing on pattern matching.
\end{itemize}

\chapter{Background}

\section{Context Free Grammars}

\chapter{Syntax Constructions}

This section details the design and motivations behind syntax constructions, the main product of this thesis. Syntax constructions are macros, similar to those in various Lisps, but with additional guarantees and capabilities. Section~\ref{sec:design-goals} will reiterate and clarify the goals of the design to guide the later sections (\ref{sec:constructions-and-types} through \ref{sec:design-implementation}), which will build the constructions one piece at a time.

\section{Design Goals} \label{sec:design-goals}

The goals that guide the design of syntax constructions can be found below. Note that not all of these are fully met.

\begin{description}
  \item[Tower of languages:] It should be possible to define new languages in terms of other languages, very much akin to Lisp tradition.
  \item[Syntactical freedom:] Syntax constructions should be able to specify new syntax, ideally with no constraints.
  \item[Abstraction preservation:] No usage of a syntax construction should expose its implementation to an end user.
  \item[Good error messages:] Improper use or implementation of syntax constructions should present the user with understandable error messages.
  \item[Composition through cherrypicking:] It should be possible to reuse individual features of languages, i.e., the unit of composition must be smaller than a full language.
  \item[Reasoning without context:] Expanding upon the previous point, a syntax construction should be as self contained as possible to permit reasoning about it without full awareness of its context.
\end{description}

\section{Constructions and Types} \label{sec:constructions-and-types}

Syntax constructions center around two concepts, the constructions themselves and their types. A syntax construction is essentially a macro with some additional features and guarantees, while a syntax type is something along the lines of \mintinline{haskell}{Expression}, \mintinline{haskell}{Statement} or \mintinline{haskell}{Pattern}.

A singular syntax construction forms the unit of composition; they can be included or excluded in a language on an individual basis.

\section{Syntax} \label{sec:design-syntax}

This section will use the definition of a simple arithmetic language as a running example (see below). The language contains addition, multiplication, grouping through parenthesis, integer literals and vector literals.

\begin{minted}{text}
syntax type Expression

syntax addition:Expression =
  a:Expression "+" b:Expression {
    #prec 11
    #assoc left
    <...>
}

syntax multiplication:Expression =
  a:Expression "*" b:Expression {
    #prec 12
    #assoc left
    <...>
}

syntax parens:Expression = "(" e:Expression ")" {
    <...>
}

syntax intLit:Expression = i:Integer {
    <...>
}

syntax vecLit:Expression =
  "[" e:Expression ("," es:Expression)* "]" {
    <...>
}
\end{minted}

Note that the implementation of each syntax construction is omitted and will instead be discussed in section~\ref{sec:design-implementation}.

Parsing consists of two steps: lexing and the actual parsing. The former uses a fixed lexer that produces five kinds of tokens: integers, real numbers, strings, identifiers and symbols. The lexical syntax of these are chosen to align as well as possible with the syntax of common programming languages, which works well enough in most cases. It does however represent a clear area of possible improvement.

Note also that the lexer produces no keywords. Instead any syntax construction that requires a keyword is expected to include a quoted string (e.g. \mintinline{text}{"for"}) which will match a specific token (e.g. an identifier with the text ''for'').

The actual parsing is guided by the syntax constructions. If we for a moment disregard precedence and associativity there is a very clear connection between syntax constructions and a context free grammar: each construction is a production, while each syntax type is a non-terminal. To include precedence and associativity we do a standard transformation and introduce an extra non-terminal for each precedence level, producing the following grammar:

\setlength{\grammarindent}{8em}
\begin{grammar}
<Expression> ::= <Expression> '+' <Expression1>
  \alt <Expression1>

<Expression1> ::= <Expression1> '*' <Expression2>
  \alt <Expression2>

<Expression2> ::= '(' <Expression> ')'
  \alt <Integer>
  \alt '[' <Expression> (',' <Expression>)* ']'
\end{grammar}

\begin{grammar}
<Expression> ::= <Expression> '+' <Expression>
  \alt <Expression> '*' <Expression>
  \alt '(' <Expression> ')'
  \alt <Integer>
\end{grammar}

\synt{Integer} matches any single integer token. Note the presence of the extended BNF operator * in the final production, present also in the \mintinline{text}{vecLit} syntax construction. Syntax constructions can use + (one or more), * (zero or more) and ? (zero or one) as conveniences. Later on, in section~\ref{sec:design-implementation}, the use of EBNF operators will have a larger impact, but for now they are mereply conveniences.

The subject of precedence bears further elaboration. It is required in some form or other, otherwise a very large class of languages would be inexpressible, e.g. $a + b * c$ would be ambiguous. However, precedence only has meaning when comparing multiple syntax constructions to each other, which is in direct conflict with the design goal of only requiring a user to consider each syntax construction in isolation. The current solution is a trade-off and does not require explicit comparison with other constructions, precedence is simply represented as a number, but it does present an implicit dependence on all other syntax constructions of the same syntax type. Other options might include explicitly stating one syntax construction as preferred over another, similar to SDF \cite{Heering1989}.

\subsection{Ambiguity}



\section{Bindings} \label{sec:design-bindings}

To support the goal of abstraction preservation and good error messages syntax constructions include name binding semantics.

To motivate the design of the binding semantics this section will examine three examples.

\begin{listing}
\begin{minted}{ocaml}
let a = "value" in
print_string a
\end{minted}
\caption{An example in OCaml demonstrating simple let bindings.}
\label{lst:nested-binding-example}
\end{listing}

Listing~\ref{lst:nested-binding-example} demonstrates simple let bindings. A let binding introduces bound names to a sub-tree (\mintinline{ocaml}|a| is bound in \mintinline{ocaml}|print_string a| in this case). The syntax construction below codifies these semantics.

\begin{minted}{text}
syntax let:Expression = "let" x:Identifier "=" e:Expression "in" body:Expression {
  #bind x in body
  <...>
}
\end{minted}

The syntax type \mintinline{text}|Identifier| is a built in type that participates in name binding. \mintinline{text}|#bind x in body| specifies that \mintinline{text}|x| is part of a definition, otherwise it would be interpreted as a reference that must be bound in the current environment.

Listing~\ref{lst:imperative-binding-example} demonstrates scoping rules in Java. Variables cannot be used before their declaration nor after the scope they were introduced in ends.

\begin{listing}
\begin{minted}{java}
String first = "first";
{  // Opens a new scope
  String second = "second";
  System.out.println(first + second + third);  // Error: cannot find third
  String third = "third";
  System.out.println(first + second + third);
}  // Closes the new scope
System.out.println(first + second + third);  // Error: cannot find second or third
\end{minted}
\caption{An example in Java demonstrating scopes and imperative style local variables.}
\label{lst:imperative-binding-example}
\end{listing}

These semantics can be described using only capabilities introduced so far, but with some drawbacks. For example, variable declaration, scope introduction and method call could be specified as follows:

\begin{minted}{text}
syntax declaration:Statement = t:Type x:Identifier "=" e:Expression ";" next:Statement {
  #bind x in next
  <...>
}

syntax scope:Statement = "{" s:Statement "}" ";" next:Statement {
  <...>
}

syntax call:Statement = e:Expression "(" (a:Expression ("," as:Expression)*)? ")" ";" next:Statement {
  <...>
}

syntax empty:Statement = {
  <...>
}
\end{minted}

The idea is to treat each statement as if it contained the following statements as a sub-tree, allowing \mintinline{text}|declaration| to bind a name in all following statements. A single ''special'' statement, an empty one, is additionally needed after the final statement.

This specification breaks somewhat with intuition, conceptually Listing~\ref{lst:imperative-binding-example} contains a list of statements, one of which contains sub-statements, but using the specification above it only produces one syntax construction. It also introduces boilerplate, each statement must be written with an extra statement after it, regardless of whether it would normally use it or not, which breaks somewhat with the goal of considering each syntax construction in isolation.

So while bindings in sub-trees are capable of expressing the desired semantics, they leave something to be desired.

Listing~\ref{lst:javascript-function-binding-example}, however, cannot be expressed using bindings in sub-trees. A function in JavaScript can be used both before and after its declaration, permitting mutual recursion. Binding in sub-tree would then require both functions to be a sub-tree of the other, requiring the abstract syntax tree to be cyclic, which seems undesirable.

\begin{listing}
\begin{minted}{javascript}
function foo(n) {
  console.log("foo", n);
  if (n > 0) bar(n-1);
}
function bar(n) {
  console.log("bar", n);
  if (n > 0) foo(n-1);
}
\end{minted}
\caption{An example in JavaScript demonstrating mutually recursive functions.}
\label{lst:javascript-function-binding-example}
\end{listing}

Instead, a syntax construction is allowed to specify that an identifier is bound before and / or after itself, thus permitting bindings that are not limited to sub-trees. Additionally explicit scopes can be introduced to limit bindings. The syntax construction below gives has the desired binding semantics.

\begin{minted}{text}
syntax funcDecl:Statement = "function" f:Identifier "(" (a:Identifier ("," as:Identifier)*)? ")" "{" body:Statement+ "}" {
  #bind f before
  #bind f after
  #bind f, a, as in body
  #scope (body)
  <...>
}
\end{minted}

This solution once again disconnects statements, allowing each to be defined in isolation, even when they need to affect adjacent statements with a binding.

% TODO: need consistent terminology for things (expression is used to mean node sometimes, but Expression is commonly used for a different meaning, is probably not a very good thing.)
% TODO: this almost certainly needs a different title, since it's not about the implementation of the system, rather about the implementation of a syntax construction
\section{Implementation} \label{sec:design-implementation}

Syntax constructions, being essentially macros, need a way to specify how they expand into whatever underlying language they are implemented on top of. Additionally, the goal of abstraction preservation requires that the implementation and expansion is never exposed to an end user. As such, the expansion must never fail or produce a malformed program.

To achieve this the expansion is specified in a fairly limited way, which makes checking the implementation simpler.

Furthermore, the system requires a base language, some point at which point expansion is finished. Syntax constructions themselves pose no requirements on this language and instead allow a construction to be marked as ''builtin'', meaning part of the base language.

As expansion proceeds and syntax constructions are replaced by syntax constructions closer to the base language a similar transformation will take place on syntax types, a program will go from using the types of its language, to the types of the host language, etc. all the way down to the base language. To ensure that the final expanded program is syntactically correct each syntax type must designate either its underlying type or that it is a syntax type of the base language, i.e. ''builtin''. For example:

\begin{minted}{text}
syntax type BaseExpression = builtin

syntax lambda:BaseExpression = "\" x:Identifier "." e:Expression {
  #bind x in e
  builtin
}
\end{minted}

In the simplest case an implementation is simply a syntactical expression in the target language, with elements of the original syntax construction spliced in, similar to quote and unquote capabilities in similar systems. For example:

\begin{minted}{text}
syntax type Expression = BaseExpression

syntax let:Expression = "let" x:Identifier "=" e:Expression "in" body:Expression {
  #bind x in body
  #scope (body)
  #scope (e)
  BaseExpression` (\`id(x). `t(body)) `t(e)
}
\end{minted}

Here a let expression is expanded into a lambda expression and application. \mintinline{text}|Expression`|, \mintinline{text}|`id| and \mintinline{text}|`t| assist with disambiguation in parsing. The underlying type of expressions must agree. It is worth noting here that during expansion all sub-expressions (e.g. \mintinline{text}|x| and \mintinline{text}|body| above) are treated as atomic, no inspection or modification is permitted.

If the original syntax construction contains EBNF operators some elements may occur multiple times, forming lists of syntactical elements. These are included in the implementation using folds:

\begin{minted}{text}
syntax switch:Expression = "switch" e:Expression "{" cases:("case" test:Expression ":" result:Expression)* "default" ":" default:Expression "}" {
  Expression`
    let x = `t(e) in
    `t(foldr cases rest
         (Expression` if `t(test) == x
                        then `t(result)
                        else `t(rest))
         default)
}
\end{minted}

Here a switch is translated to a simple series of ifs, checking for equality with each case in turn. The fold expression takes four arguments: what list to fold over, the name of the result so far (the accumulator), the expression that will be used to construct the next result, and the initial value. The example uses a right fold, additionally left fold and versions without an initial value are available, the latter only being usable if the list contains at least one value (i.e. it was constructed using the ''+'' operator, not ''*'' or ''?'').

\section{Proof of Termination} \label{sec:proof-termination}

\section{Proof of Soundness} \label{sec:proof-no-errors}

\chapter{Evaluation}

\section{An Imperative Language} \label{sec:imperative-eval}

\section{A Functional Language} \label{sec:functional-eval}

\chapter{Related Work}

The related work is separated roughly based on the degree to which they solve the two problems introduced in section~\ref{sec:introduction}: solutions for abstraction problems, solutions for syntax problems, and solutions that address both problems. These solutions deal with various forms of macro systems, except for those only concerning syntax; they deal with parsing extensible syntax and largely do not concern themselves with the semantics of what is being parsed.

\section{Abstraction Solutions} \label{sec:abstraction-solutions}

Lisps have long had macro systems that do not introduce accidental name capture, mostly through renaming (TODO: references). The latest iteration of Racket's \cite{plt-tr1} macro expander instead achieves binding hygiene by using sets of scopes \cite{Flatt:2016:BSS:2837614.2837620}. Somewhat simplified, a set of scopes is attached to each identifier, namely the scopes that contain it. To find the binding referred to by an identifier with set $s_i$, find the binding with set $s_b \subseteq s_i$ such that for all other sets $s'_b \subseteq s_i$ attached to binders, $s'_b \subset s_b$, i.e. $s_b$ is the largest subset of $s_i$. The authors report the implementation as simpler to follow, and while ambiguous references are possible they do not appear in practice.

$\lambda_m$ \cite{Herman2010} takes a different route and is an extension of the lambda calculus with macros and macro type signatures. These signatures describe the structure of macro arguments and results, including binding structure, and form an inspirational basis to the approach taken by this thesis in regard to hygiene and name binding.

Some notable differences exist however:
\begin{itemize}
  \item Bindings in $\lambda_m$ macros are always nested, in the sense that bindings are never available outside of the macro that introduced them. In contrast, consider a local variable in e.g. C:
  \begin{minted}{c}
  int foo = 4;
  printf("foo: %d\n", foo);
  \end{minted}
  The local variable \mintinline{c}{foo} is available for the remainder of the scope, potentially long after the declaration has ended. This thesis handles both kinds of binding.
  \item TODO: EBNF, recursion and union types
\end{itemize}

In addition, $\lambda_m$ introduces definitions of $\alpha$-equivalence and hygiene that do not depend on macro expansion and places focus on allowing reasoning about unexpanded programs.

Romeo \cite{Stansifer2014} continue the path of typed macros but extends it to allow procedural macros, as opposed to the pattern matching and replacing of $\lambda_m$.

\section{Syntax Solutions} \label{sec:syntax-solutions}

The syntax definition formalism SDF \cite{Heering1989} is a system with a combined notation for lexical and context-free grammar, along with a parser. Productions are listed individually, allowing non-terminals to be spread across files in a very similar fashion to the syntax constructions of this thesis. SDF also uses associativity annotations for operators instead of necessitating a manual rewriting of the grammar.

A specification in SDF is used to generate a lexical grammar and a context-free grammar, both of which are then used for the actual parsing. The productions in the specification also double to describe an abstract syntax tree, which is the end result.

SDF has also been extended to handle layout-sensitive grammars \cite{Erdweg2013} through extra annotations, thus retaining the declarative nature of a grammar.

\textcite{Silkensen2013} instead consider the problem of combining already constructed grammars in a scalable way. The main observation they use is that most domain specific languages deal with values of different types. With this in mind the different grammars can use these types, for example using non-terminals such as \mintinline{text}{Matrix} instead of \mintinline{text}{Expression}. Additionally, if identifiers can be specified to have a specific type (e.g. \mintinline{text}{Matrix}) they can be used as islands in island parsing. By using these two things the authors present a parsing algorithm that needs to examine far fewer possible parsings, even in the presence of many DSLs. The precise scalability claim can be found in the paper.

\section{Combined Solutions} \label{sec:full-solutions}

SoundX \cite{Lorenzen2016} is a system for specifying extensible languages. It uses SDF (see section \ref{sec:syntax-solutions}) to specify syntax and adds type rules and type judgements. A language extension is specified as a set of rewritings (macros) and type rules for the rewritings. Macros in SoundX are checked to guarantee that they preserve abstraction on a type level, i.e. they introduce no type errors during rewriting. Similarly, code can be type checked without performing rewriting.

The system works through a rewriting of derivation trees, not pure syntax, thus macros have access to not only types explicit in a program but also derived types.

Copper \cite{VanWyk2007} and Silver \cite{Van-Wyk2010Silver:-An-exte} together form a system for extensible languages based on attribute grammars. Copper defines a grammar and a lexical scanner that work in tandem, where the scanner only returns tokens that the grammar defines as valid next tokens given the current state of the parse. The combination mananges to parse more languages, since the scanner can work unambiguously in more cases. Silver allows modular language extensions on some host language, all of them defined using attribute grammars, along with some guarantees on their behaviour under composition \cite{Kaminski2017Reliably-compos}.

\chapter{Results and Discussion}

\section{Results}

\section{Discussion}

\section{Conclusion}

\printbibliography[heading=bibintoc]

% \appendix

% \chapter{Unnecessary Appended Material}

\end{document}
