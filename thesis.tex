\documentclass{kththesis}

\usepackage{minted}

\usepackage{syntax}
\usepackage{tabu}
\usepackage{amsmath}
\usepackage{semantic}
\usepackage{graphicx}
\usepackage{parskip}

\usepackage{algpseudocode}

\usepackage{csquotes} % Recommended by biblatex
\usepackage{biblatex}
\addbibresource{references.bib} % The file containing our references, in BibTeX format

\usepackage{hyperref}

\title{Building Programming Languages, Construction by Construction}
\alttitle{Att bygga programmeringsspråk, konstruktion för konstruktion}
\author{Viktor Palmkvist}
\email{vipa@kth.se}
\supervisor{David Broman}
\examiner{Mads Dam}
\programme{Master in Computer Science}
\school{School of Computer Science and Communication}
\date{\today}

\raggedright
% \setlength{\parindent}{1.5em}

\begin{document}

% Frontmatter includes the titlepage, abstracts and table-of-contents
\frontmatter

\titlepage

\begin{abstract}
This thesis introduces syntax constructions as a method of constructing programming languages as a set of language features, permitting reuse through cherry-picking single features from other languages. They are similar to macros, but with greater syntactical freedom than those found in most languages (e.g. Lisp), and extra annotations specifying their binding semantics. These annotations are then used to allow full name resolution before macro-expansion, along with a check to ensure that no syntax construction can introduce a new binding error trough its expansion. This means that error messages pertaining to name binding are presented in terms of code the programmer actually wrote, instead of code generated through several layers of macro-expansions.

Syntax constructions also use syntactical ambiguity as a feature through informative error messages, but the tools provided to control the ambiguity and where it appears prove insufficient. Ultimately syntax constructions represent a step towards a certain user experience in creating new programming languages, but are themselves not the desired solution.
\end{abstract}


\begin{otherlanguage}{swedish}
\begin{abstract}
Denna avhandling introducerar syntax-konstruktioner, en metod för att konstruera programmeringsspråk som en samling mindre konstruktioner. Detta gör det möjligt att återanvända enstaka konstruktioner från redan implementerade språk. Syntax-konstruktioner liknar macron, men med större syntaktisk flexibilitet och ett sätt att specificera hur de interagerar med namnbindning. Det sistnämnda möjliggör analys av namnbindningar utan att expandera macron och ett sätt att kontrollera att en given macro aldrig introducerar namnbindningsfel. Detta medför att när felmeddelanden behöver presenteras så kan de referera till kod som en användare faktiskt skrivit, istället för kod som genererats vid expandering.

Syntax-konstruktioner låter en användare skapa syntaktisk ambiguitet i sitt programmeringsspråk och försöker få en bra användarupplevelse med hjälp av informativa felmeddelanden. De verktyp som tillhandahålls för att kontrollera detta visar sig dock vara otillräckliga, vilket medför att syntax-konstruktioner i sig själva inte är en tillräcklig lösning för den tilltänkta användarupplevelsen vid programspråkskonstruktion, utan enbart ett steg på vägen.
\end{abstract}
\end{otherlanguage}


\tableofcontents


% Mainmatter is where the actual contents of the thesis goes
\mainmatter

\chapter{Introduction} \label{sec:introduction}

% TODO: list of examples of "a wide array of problems"
The computational power of a modern computer makes it an attractive tool to apply to a wide array of problems. Their power does not come with intelligence however (advancement in machine learning notwithstanding), the machine merely does what it is told, no more, and no less. This becomes a problem because the instructions that a computer understands are very basic, things like arithmetic, comparisons and bit-twiddling, not the high level concepts we as humans would use to consider problems.

Enter programming languages: languages designed to be written by a human and then translated into instructions understood by a machine. These immediately elevate our ability to instruct a computer using more high level concepts, bringing the discourse somewhat closer to that of the problems we wish to solve.

Most programming languages in use are so-called general purpose programming languages, languages intended to be useful for any kind of computation. Common examples include C, Java, and Python. Their generality and freedom are their strength and their weakness; they can solve many problems, but those solutions require much in ways of code. Just like machine code deals with largely arithmetic operations, general purpose programming languages deal with data structures and expressing algorithms; they deal with computer science.

The problems we wish to solve might not reside in the domain of computer science however, we might be solving something in e.g. biology, physics, or science. A general purpose programming language requires that the solution uses terms from computer science, not the problem domain.

There is an alternative here of course: a programming language that is not general purpose but instead domain specific. Such a language is naturally far less generally applicable, but it allows stating the solution in the same domain as the problem. This is likely to make it easier to solve the problem, and limits repeated work; the implementation of the programming language can solve many of the computational aspects and efficiency problems once, instead of requiring each solution in the same domain to re-solve these peripheral problems.

So a domain specific language can make it easier to solve the problem at all, and on top of that do it more efficiently. What is the catch? In some cases there might not be one, but more frequently at least one of three things is true:

\begin{itemize}
  \item There is no language for the given domain.
  \item There is no well-designed language for the given domain.
  \item There is no well-implemented language for the given domain.
\end{itemize}

The reason is simple: making a programming language is a lot of hard work. It requires a wealth of computer science knowledge and engineering to provide a good implementation and a usable tool, but it also requires a wealth of knowledge of the problem domain to design a good language. The work required to create a good language to solve many problems with is---with a very, very high likelihood---much more work than solving any one of those problems individually.

We wish it were not so, and thus much research has been done in the area of making domain specific languages. This thesis in particular builds on the foundation of macros as a method of creating domain specific languages.

The remainder of this introduction will provide additional context and motivation for the thesis and the associated approach (Sections \ref{sec:compiler-design} through \ref{sec:desired-ambiguity}), a more precise problem statement (Section~\ref{sec:research-question}), and a list of contributions (Section~\ref{sec:contributions}).

\section{Programming Language Implementation} \label{sec:compiler-design}

The implementation of a programming language---be it an interpreter or a compiler---tends to be divided into phases, each dealing with their own aspect of the language. For example, a parser details the syntax of the language, while name resolution handles names, scoping, and namespaces. A type checker details part of the semantics, what language constructs can be combined, then code generation or a simple interpreter codify the runtime semantics of the individual constructs. This means that the implementation of any single language construct is spread throughout the language implementation, necessitating it be considered as a whole. Adding a new construction requires changes in many places.

This also means that reuse of a language implementation is difficult; each phase makes assumptions on what has been done before it and what will be done later, trying to use or replace a single phase gives little flexibility to make something different than what is already there.

Attempting to use a single language construct from a language is likewise difficult, the various aspects of its definition are spread throughout the different phases, quite possibly intertwined with code handling other constructs that happen to be similar in that particular language.

Despite this there are some quite successful cases of language implementation reuse:

\begin{description}
  \item[JVM] A large number of programming languages exists that can compile to Java bytecode, enabling them to reuse the implementation originally intended for Java. Examples include Scala, Clojure and Kotlin. Other languages have the JVM as an additional backend, for example Ruby (JRuby) and Python (Jython). % TODO: refs
  \item[JavaScript] Many languages can compile to JavaScript, thus reusing the entire language while adding something new on top. Examples include TypeScript (essentially JavaScript with static types), CoffeeScript (alternative syntax for JavaScript), and Elm (statically typed language with JavaScript as a compile target).
  \item[OCaml] ReasonML replaces the parser and surrounding tooling of OCaml, providing a different syntax to what is otherwise the same language.
  \item[LLVM] The LLVM project (previously an acronym for low level virtual machine, now a name all by itself) defines an intermediate representation that can be used both as a target for a language and as a source for translation into machine code. This allows changing either frontend or backend while reusing the other, as well as a large number of optimizations defined on the intermediate representation. Languages using LLVM either by default or as part of an alternative backend include C, C++, Rust, and Haskell. Examples of instruction sets to which LLVM intermediate representation can be translated include x86, x86-64, ARM, and PowerPC.
\end{description}

Somewhat simplified, the key common characteristic of these examples is that the reused part tends to be from somewhere in the middle until the end of the original implementation; the replaced part has the first few phases, then all remaining phases are from the original implementation. No one only replaces a few phases in the middle.

While this does allow reuse of large chunks of an implementation, much still has to be reimplemented. The different implementations of the first phases generally do much of the same work, all of them parse their language, then do name resolution, then maybe some type checking, and so on. Many details will differ, but the basic kind of work tends to stay the same.

We would ideally like to have more flexible forms of reuse, enabling reuse of singular language features, and not requiring reimplementation of name resolution and many other common tasks beyond specifying the particular details of the current language.

\section{Macros}

A macro in a programming language context is a piece of syntax with a translation into some other syntax. This translation is specified by a user of the language as opposed to the original language implementer, which gives an opportunity to define new syntactical constructs and languages without having to change the language implementation or make a new language entirely.

To be a bit more concrete, the following example is in Racket\cite{Flatt2010Reference:-Rack}, a dialect of lisp:

% TODO: check that the example code is actually correct and runnable
\begin{minted}{racket}
; This is a shortcutting 'and', i.e. 'b' will
; only be evaluated if 'a' is true
(define-syntax-rule (and a b)
  (if a b #f))

; Assuming the existence of a function trace
; that prints its argument and then returns
; it, we can demonstrate 'and' like so:
(and (trace #t) (trace #f))
; => #f
; This prints #t, but not #f, i.e. (trace #f)
; was not evaluated
\end{minted}

This would not be possible to implement as a function in Racket since functions require their arguments to be fully evaluated before they are called, i.e. the second argument would be evaluated before \mintinline{racket}{and}, regardless of the value of the first argument.

This particular example shows a simple example where the arguments of the macro are simply inserted into a form of template, but macros in general can be far more complex and powerful. Macros can utilize recursion as well as the full power of lisp, i.e. a macro in general is a function from syntax to syntax that runs at compile-time.

The expressive power provided here is great; any arbitrary computation can be done at compile time, for example type checking (typed racket \cite{Tobin-Hochstadt:2011:LL:1993498.1993514}), destructuring and pattern matching (Clojure, standard library\footnote{https://github.com/clojure/clojure/blob/master/src/clj/clojure/core.clj} and external library\footnote{https://github.com/clojure/core.match} respectively), and coroutines (Clojure\footnote{https://github.com/clojure/core.async}). Racket (which was used in the example above) takes this one step further and describes itself as an ''ecosystem for developing and deploying new languages'', all of which is powered by its macro system.

There are some drawbacks however, and we will spend the next few sections examining some of them.

\subsection{Problems of Abstraction} \label{sec:problem-abstraction}

The abstractions provided by macros can easily become leaky abstractions without special care.

\subsubsection{Name Capture}

A naive macro expander using textual replacement will introduce accidental name capture. For example, given the following macro (using Racket syntax) intended to calculate $a + a + b$ while computing $a$ only once:

\begin{minted}{racket}
(define-syntax-rule (double-add a b)
  (let ([x a])
    (+ x x b)))
\end{minted}

The following expression will exhibit accidental name capture when macro expansion uses pure textual replacement:

\begin{minted}{racket}
(let ([x 2])
  (double-add 1 x))
; expands to:
(let ([x 2])
  (let ([x 1])
    (+ x x x)))
; which evaluates to
3
; while our intended semantics would evaluate to:
4
\end{minted}

Notable here is that the above would not produce an error of any kind, the result would simply be silently wrong.

Most macro systems will avoid this problem through various forms of rewriting or more complicated forms of name resolution (e.g. \cite{FLATT2012Macros-that-Wor,Flatt2016Binding-As-Sets}), a notable exception being the C preprocessor that makes no such attempt.

\subsubsection{Errors after Expansion}

In some cases the fully expanded code is incorrect, either because the macro was poorly implemented, or because it was used improperly. For example:

\begin{minted}{racket}
(define-syntax-rule (new-let x e body)
  (let ([x e])
    body))

(new-let 2 x
  (+ 1 x))
\end{minted}

Here we define a new binding construction, but misuse it; the identifier and the value are switched. The macro expansion will succeed and produce the code below, but at that point an error message will be produced about improper use of \mintinline{racket}{let}, even though \mintinline{racket}{let} never occurs in the original code.

\begin{minted}{racket}
(let ([2 x])
  (+ 1 x))
\end{minted}

The error exposes the implementation of the macro. The generated code may be introduced through several layers of macro expansion and can be quite complicated, making the connection between the error message and the actual error even less clear.

In a statically typed language this problem has an additional form: type errors after macro expansion.

\subsection{Problems of Syntax} \label{sec:problem-syntax}

One advantage of macros is their ability to essentially introduce syntactic sugar to a language at a user level. Despite this most commonly used macro systems place some fairly strict limits on the newly introduced syntax. For example, a C preprocessor macro must be an identifier, possibly followed by an argument list, and a lisp macro must be a list of valid forms started by a symbol.

Such restrictions simplify parsing the language, since it ensures the grammar is fixed and cannot be changed during parsing.

\section{Desired Ambiguity} \label{sec:desired-ambiguity}

% TODO: not sure this section is written in quite the right way to be in the introduction, was merely moved from a discussion part at the end of the thesis

When designing a grammar to describe the syntax of a programming language ambiguity tends to be an undesirable property. There are multiple reasons for this, for example an unambiguous grammar may be faster to parse (e.g. \cite{Earley1970An-Efficient-Co} has a better worst case for unambiguous grammars), or one might consider the possibility of encountering code that, while syntactically correct, does not have a well defined meaning as a bad thing.

I would however argue that there is a place for ambiguity in programming language grammars, if we presuppose that code is read more often than it is written. Consider the expression \mintinline{python}{1 & 3 == 1}, where \mintinline{python}{&} is bitwise ''and'' and \mintinline{python}{==} is equality. The result of the expression depends on the relative precedence of the two operators. Thus a reader can know the precise semantics of all operators and values involved, and still not know what the code does.

This is of course a tradeoff, the same argument could be made for an expression such as \mintinline{python}{1 + 2 * 3}, yet requiring explicit grouping for all expressions seems excessive. The difference stems from the frequency each operator is used, e.g., most code uses basic arithmetic and logical operators, but bitwise operators are relatively infrequent. The former are also taught early in mathematics, including precedence, thus most programmers would know the exact meaning of \mintinline{python}{1 + 2 * 3} while \mintinline{python}{1 & 3 == 1} would be less obvious.

Worse, the meaning of the latter expression varies by language, even if the languages have the same meaning for the operators. The evaluation of that same expression in C and Python can be seen below.

\begin{center}
\begin{tabular}{c|c}
C & Python \\
\hline
\mintinline{c}{1 & 3 == 1} & \mintinline{python}{1 & 3 == 1} \\
\mintinline{c}{1 & (3 == 1)} & \mintinline{python}{(1 & 3) == 1} \\
\mintinline{c}{1 & 0} & \mintinline{python}{1 == 1} \\
\mintinline{c}{0} & \mintinline{python}{True} \\
\end{tabular}
\end{center}

Thus a programmer who knows the precedence in one language may read the code, understand the semantics of all operators involved, and yet misunderstand the semantics of the expression.

My suggestion then is the following: if most programmers would agree on the interpretation of some code it should be parsed unambiguously as such. If the correct interpretation is non-obvious then the programmer writing the code should be asked to clarify their intent, most likely by using parentheses for explicit grouping. The latter can be achieved with an ambiguous grammar and good error messages (Section~\ref{sec:errors-ambiguous} shows a good first step towards such error messages).

The key then is to provide tools with which a language designer has ample control of where ambiguity is surfaced to an end user. This control should deal with both ambiguity and unambiguity, neither should appear accidentally when the intent was the opposite.

% TODO: this next paragraph should almost certainly not be here, we haven't talked about syntax constructions yet. Reread agrees with this.

Syntax constructions allow for ambiguity, but provide poor tools for controlling it (see Section~\ref{sec:static-dynamic-ambiguity}) and sometimes require extensive rewrites to remove undesired ambiguity (see Section~\ref{sec:ambiguous-lists}).
% TODO: this doesn't quite feel like an end
% TODO: should probably have some more mention of what syntax constructions give here?

\section{Problem Statement} \label{sec:research-question}

This thesis explores programming language construction through the creation and composition of individual language features that are self contained in terms of syntax and semantics. In particular, it should be possible to extend a language by cherrypicking one or a few features from another language without changing them.

Features here refers to things such as an if-statement, an anonymous function, or a pattern used in pattern matching.

The goal above states very little in regards to the form of a solution, nor does it provide much assistance in evaluating the usefulness of any particular solution. To alleviate this issue the following additional design goals will guide the design and evaluation of the approach.

\begin{description}
  \item[Tower of languages:] It should be possible to define new languages in terms of other languages, very much akin to Lisp tradition, as a form of reuse.
  \item[Syntactical freedom:] Language features should be able to specify new syntax, ideally with no constraints.
  \item[Abstraction preservation:] No usage of a language feature should expose its implementation to an end user.
  \item[Good error messages:] Improper use or implementation of language features should present the user with understandable error messages.
  \item[Composition through cherrypicking:] It should be possible to reuse individual features of languages, i.e., the unit of composition must be smaller than a full language.
  \item[Reasoning without context:] Expanding upon the previous point, a language feature should be as self contained as possible to permit reasoning about it without full awareness of its context.
\end{description}

These goals are referred to throughout the thesis to motivate choices or highlight strengths and flaws with the chosen approach.

\subsection{Delimitations} \label{sec:delimitations}

\begin{description}
  \item[Type safety] Type safety is not considered. Incorrect usage of features that would give rise to type errors is not detected and will most likely present errors in terms of the implementation of the features being (mis)used. This affects abstraction preservation and good error messages.

  \item[Lexing] This thesis assumes that parsing is done on a token stream, i.e. the syntactical freedom of a feature is limited by a predefined tokenization. This affects syntactical freedom. In practice it mostly means that the syntax of the following things cannot be customized:
  \begin{itemize}
    \item Valid identifiers
    \item Literals, e.g. integers and strings
    \item Comments
    % \item Symbols, the lexer assumes that certain characters will never be part of multi-character symbols, e.g. \mintinline{syncon}{"<;>"} will be tokenized as three symbols since \mintinline{syncon}{";"} is assumed to always be alone. % TODO: is this really important to mention here? it seems too specific
  \end{itemize}

  \item[Indentation] This thesis assumes syntax to be whitespace independent, which precludes defining languages such as Python where indentation is important. This affects syntactical freedom.

  \item[Namespaces and Importing] Namespaces and other similar methods of partitioning definitions and later importing them are not considered, only lexical scoping is used. This is sufficient for C and some similar languages, but not for e.g. Java. It does not explicitly affect any of the design goals, but it clearly limits the languages that can be designed.
\end{description}

\section{Contributions} \label{sec:contributions}

This thesis contributes the following:

\begin{itemize}
  \item A method of specifying the syntax, binding semantics and expansion of a language construction (macro) in a singular location. The design considerations from a user perspective can be found in Chapter~\ref{sec:syntax-constructions}, while formalization is in Chapter~\ref{sec:formalization}.
  \item Proof that macro expansion using the above method terminates, given a finite input (Section~\ref{sec:expansion-formalization}).
  \item An algorithm for checking that a given expansion specification can never introduce binding errors, along with justification for the correctness of this algorithm (Section~\ref{sec:expansion-checking-formalization}).
  \item An implementation of a non-trivial subset of OCaml (Section~\ref{sec:functional-eval}) to evaluate the method's suitability in modeling a typical functional language, focusing on pattern matching.
  \item An implementation of a non-trivial subset of Lua (Section~\ref{sec:imperative-eval}) to evaluate the method's suitability in modeling a typical imperative language.
  \item Extensive evaluation of strengths and weaknesses of the method (Chapter~\ref{sec:evaluation}).
\end{itemize}

\section{Ethics, the Environment, and Societal Impact}
% TODO: this has somethings that are essentially societal, but they weren't written as that, ensure that it is properly covered

Given the nature of this thesis, it has no direct ethical or environmental impact, but plenty of indirect impact. The aim is to produce a convenient way of constructing new programming languages, i.e., the product of this thesis is a tool to construct tools (programming languages) to construct tools (programs). These final tools, programs, can of course be ethically questionable, or perfectly ethical, but considering this at such a removed level is\ldots impractical.

On a more direct level however, the purpose is essentially to make programming language construction easier, i.e. making the work of a certain subset of people easier, which seems like an morally positive thing to do.

Environmentally speaking, the most clear impact would be the power consumption required to run a program. This thesis makes no claim or effort on execution efficiency (though it is part of plans for future work), thus there is a likelihood that the programs produced would be inefficient, and thus have a high power consumption. Additionally, by making programs and programming languages easier to construct we increase the likelihood that programming will be used for some problem where it would previously have been implausible. The impact of this is unclear and depends on a comparison with the impact of whatever would otherwise have been used, or nothing if the problem would not have been solved at all.

\chapter{Related Work}

The related work is separated roughly based on the degree to which they solve the two problems introduced in sections \ref{sec:problem-abstraction} and \ref{sec:problem-syntax}: solutions for abstraction problems, solutions for syntax problems, and solutions that address both problems. These solutions deal with various forms of macro systems, except for those only concerning syntax; they deal with parsing extensible syntax and largely do not concern themselves with the semantics of what is being parsed.

\section{Abstraction Solutions} \label{sec:abstraction-solutions}

Lisps have long had macro systems that do not introduce accidental name capture, mostly through renaming \cite{FLATT2012Macros-that-Wor}(TODO: more references). The latest iteration of Racket's \cite{Flatt2010Reference:-Rack} macro expander instead achieves binding hygiene by using sets of scopes \cite{Flatt2016Binding-As-Sets}. Somewhat simplified, a set of scopes is attached to each identifier, namely the scopes that contain it. To find the binding referred to by an identifier with set $s_i$, find the binding with set $s_b \subseteq s_i$ such that for all other sets $s'_b \subseteq s_i$ attached to binders, $s'_b \subset s_b$, i.e. $s_b$ is the largest subset of $s_i$. The authors report the implementation as simpler to follow than the previous version using renaming, and while ambiguous references are possible they do not appear in practice.

$\lambda_m$ \cite{Herman2010A-Theory-of-Typ} takes a different route and is an extension of the lambda calculus with macros and macro type signatures. These signatures describe the structure of macro arguments and results, including binding structure, and form an inspirational basis to the approach taken by this thesis in regard to hygiene and name binding.

There is a rather notable difference however: bindings in $\lambda_m$ macros are always nested, in the sense that bindings are never available outside of the macro that introduced them. In contrast, consider these declarations in Haskell:
\begin{minted}{haskell}
even n = case n of
  0 -> True
  1 -> False
  n -> odd (n - 1)
odd n = case n of
  0 -> False
  1 -> True
  n -> even (n - 1)
\end{minted}
The functions \mintinline{haskell}{even} and \mintinline{haskell}{odd} are available both before and after their declarations, not only in some nested expression.

In addition, $\lambda_m$ introduces definitions of $\alpha$-equivalence and hygiene that do not depend on macro expansion and places focus on allowing reasoning about unexpanded programs.

Romeo \cite{Stansifer2014Romeo} continue the path of typed macros but extends it to allow procedural macros, as opposed to the pattern matching and replacing of $\lambda_m$.

\section{Syntax Solutions} \label{sec:syntax-solutions}

The syntax definition formalism SDF \cite{Heering1989The-syntax-defi} is a system with a combined notation for lexical and context-free grammar, along with a parser. Productions are listed individually, allowing non-terminals to be spread across files in a very similar fashion to the syntax constructions of this thesis. SDF also uses associativity annotations for operators instead of necessitating a manual rewriting of the grammar.

A specification in SDF is used to generate a lexical grammar and a context-free grammar, both of which are then used for the actual parsing. The productions in the specification also double to describe an abstract syntax tree, which is the end result.

SDF has also been extended \cite{Erdweg2013Layout-sensitiv} to handle layout-sensitive grammars through extra annotations, thus retaining the declarative nature of a grammar.

\textcite{Silkensen2013Well-Typed-Isla} instead consider the problem of combining already constructed grammars in a scalable way. The main observation they use is that most domain specific languages deal with values of different types. With this in mind the different grammars can use these types, for example using non-terminals such as \mintinline{syncon}{Matrix} instead of \mintinline{syncon}{Expression}. Additionally, if identifiers can be specified to have a specific type (e.g. \mintinline{syncon}{Matrix}) they can be used as islands in island parsing. By using these two things the authors present a parsing algorithm that needs to examine far fewer possible parsings, even in the presence of many DSLs. The precise scalability claim can be found in the paper.

\section{Combined Solutions} \label{sec:full-solutions}

SoundX \cite{Lorenzen2016Sound-type-depe} is a system for specifying extensible languages. It uses SDF (see section \ref{sec:syntax-solutions}) to specify syntax and adds type rules and type judgements. A language extension is specified as a set of rewritings (macros) and type rules for the rewritings. Macros in SoundX are checked to guarantee that they preserve abstraction on a type level, i.e. they introduce no type errors during rewriting. Similarly, code can be type checked without performing rewriting.

The system works through a rewriting of derivation trees, not pure syntax, thus macros have access to not only types explicit in a program but also derived types.

Copper \cite{Van-Wyk2007Context-aware-s} and Silver \cite{Van-Wyk2010Silver:-An-exte} together form a system for extensible languages based on attribute grammars. Copper defines a grammar and a lexical scanner that work in tandem, where the scanner only returns tokens that the grammar defines as valid next tokens given the current state of the parse. The combination manages to parse more languages, since the scanner can work unambiguously in more cases. Silver allows modular language extensions on some host language, all of them defined using attribute grammars, along with some guarantees on their behaviour under composition \cite{Kaminski2017Reliably-compos}.

\chapter{Design of Syntax Constructions} \label{sec:syntax-constructions}

This section details the design and motivations behind syntax constructions, the main product of this thesis. Syntax constructions are macros, similar to those in various Lisps, but with additional guarantees and capabilities. Section~\ref{sec:research-question} states the design goals that guide the design in this chapter. Sections \ref{sec:design-syntax} through \ref{sec:design-implementation} use these goals to build syntax constructions, one piece at a time.

The algorithms required to implement the semantics of syntax constructions are described to some extent here, as relevant for the design, but are described in more detail in Chapter~\ref{sec:formalization}. This chapter is mostly written for a language designer using syntax constructions, while Chapter~\ref{sec:formalization} contains more precise semantics.

\section{Constructions and Types} \label{sec:constructions-and-types}

Syntax constructions center around two concepts, the constructions themselves and their types. A syntax construction is essentially a macro with some additional features and guarantees, while a syntax type is something along the lines of \mintinline{syncon}{Expression}, \mintinline{syncon}{Statement} or \mintinline{syncon}{Pattern}. These types should facilitate statements like: ''a sum is an expression and consists of an expression, a plus, and another expression''.

A singular syntax construction forms the unit of composition; they can be included or excluded in a language on an individual basis. A language is thus a set of syntax constructions.

\section{Syntax} \label{sec:design-syntax}

This section will use the definition of a simple arithmetic language as a running example (see below). The language contains addition, multiplication, grouping through parenthesis, integer literals and vector literals.

\begin{minted}{syncon}
syntax type Expression

syntax addition:Expression =
  a:Expression "+" b:Expression {
    #prec 11
    #assoc left
    <...>
}

syntax multiplication:Expression =
  a:Expression "*" b:Expression {
    #prec 12
    #assoc left
    <...>
}

syntax parens:Expression = "(" e:Expression ")" {
    <...>
}

syntax integerLiteral:Expression = i:Integer {
    <...>
}

syntax vectorLiteral:Expression =
  "[" e:Expression ("," es:Expression)* "]" {
    <...>
}
\end{minted}

A syntax construction is defined by the following things:
\begin{itemize}
  \item Its name (e.g. \mintinline{syncon}{addition}).
  \item Its syntax type (e.g. \mintinline{syncon}{Expression}).
  \item A syntax description (e.g. \mintinline{syncon}{a:Expression "+" b:Expression}).
  \item Some (optional) extra data (e.g. precedence via \mintinline{syncon}{#prec} and associativity via \mintinline{syncon}{#assoc}).
  \item An expansion specification, here omitted and instead represented by \mintinline{syncon}{<...>}. See Section~\ref{sec:design-implementation} for more information.
\end{itemize}

This section deals mostly with the syntax description and its interaction with parsing.

% TODO: note that the lexer limitation is not fundamental, it's just a limitation of the implementation / the easiest thing that is good enough for now.
Parsing consists of two steps: lexing and the actual parsing. The former uses a fixed lexer that produces five kinds of tokens: integers, real numbers, strings, identifiers and symbols. The lexical syntax of these are chosen to align as well as possible with the syntax of common programming languages, which works well enough in most cases. It does however represent a clear area of possible improvement.

Note also that the lexer produces no keywords. What would normally have been a keyword is instead lexed as an identifier, which a syntax construction can match against by using a quoted literal in the syntax description. The built in syntax type \mintinline{syncon}{Identifier} then matches any identifier that never appears in a quoted literal, effectively emulating reserved keywords.

The actual parsing is then guided by the syntax constructions. If we for a moment disregard precedence and associativity there is a very clear connection between syntax constructions and a context free grammar: each construction is a production, while each syntax type is a non-terminal.

\setlength{\grammarindent}{8em}
\begin{grammar}
<Expression> ::= <Expression> '+' <Expression>
  \alt <Expression> '*' <Expression>
  \alt '(' <Expression> ')'
  \alt <Integer>
  \alt '[' <Expression> (',' <Expression>)* ']'
\end{grammar}

\synt{Integer} matches any single integer token. Note the presence of the extended BNF operator * in the final production, present also in the \mintinline{syncon}{vectorLiteral} syntax construction. Syntax constructions can use ''+'' (one or more), ''*'' (zero or more) and ''?'' (zero or one) as conveniences. Later on, in Section~\ref{sec:design-implementation}, the use of EBNF operators will have a larger impact, but for now they are merely conveniences.

This grammar is quite ambiguous, but the syntax constructions above contain additional information we can use to construct an unambiguous grammar instead: precedence and associativity. Including precedence and associativity in a grammar is a rather well known transformation that adds an extra non-terminal for every precedence level, like so:

\setlength{\grammarindent}{8em}
\begin{grammar}
<Expression> ::= <Expression> '+' <Expression1>
  \alt <Expression1>

<Expression1> ::= <Expression1> '*' <Expression2>
  \alt <Expression2>

<Expression2> ::= '(' <Expression> ')'
  \alt <Integer>
  \alt '[' <Expression> (',' <Expression>)* ']'
\end{grammar}

The precedence and associativity transformation above is slightly extended from the common one: it applies to any production, not only to operators. Precedence affects recursive uses of the same syntax type (i.e. using \mintinline{syncon}{Expression} in an \mintinline{syncon}{Expression}), regardless of position in the syntax description, by only allowing recursing to the same level or the next. Associativity further affects this by allowing recursion to the same level only in the leftmost recursion (with left associativity) or the rightmost recursion (with right associativity).

This expansion was chosen to align with the normal definition of operator precedence, but to not be limited to only operators, to provide a tool for disambiguation in general.

The subject of precedence bears further elaboration. It is required in some form or other, otherwise a very large class of languages would be inexpressible, e.g. $a + b * c$ would be ambiguous. However, precedence only has meaning when comparing multiple operators to each other, which is in direct conflict with the design goal of only requiring a user to consider each syntax construction in isolation. The current solution is a trade-off and does not require explicit comparison with other constructions, precedence is simply represented as a number, but it does present an implicit dependence on all other syntax constructions of the same syntax type. Other options might include explicitly stating one syntax construction as preferred over another, similar to SDF \cite{Heering1989The-syntax-defi}.

A reader with previous experience of context free grammars may have some qualms at this point; if the syntactical part of syntax constructions is just context free grammars with some rewriting help, won't ambiguous grammars be an issue? Context free grammars are not unambiguous in the general case, and determining whether a given grammar is ambiguous is not always obvious. Syntax constructions provide no static guarantees on ambiguity, but provides good error messages when ambiguous code is encountered. See sections \ref{sec:implementation-ambiguity-detection} and \ref{sec:errors-ambiguous} for implementation and evaluation of these errors, respectively, and sections \ref{sec:functional-eval} (specifically p.~\pageref{sec:ambiguous-lists}) and \ref{sec:imperative-eval} (specifically p.~\pageref{sec:lua-func-call-precedence}), amongst others, for consequences and evaluation.

\section{Bindings} \label{sec:design-bindings}

\begin{listing}[h]
\begin{minted}{ocaml}
let a = "value" in
print_string a
\end{minted}
\caption{An example in OCaml demonstrating simple let bindings.}
\label{lst:nested-binding-example}
\end{listing}

To support the goal of abstraction preservation and good error messages syntax constructions include name binding semantics.

To motivate the design of the binding semantics this section will examine three examples.

Listing~\ref{lst:nested-binding-example} demonstrates simple let bindings. A let binding introduces bound names to a sub-tree (\mintinline{ocaml}{a} is bound in \mintinline{ocaml}{print_string a} in this case). The syntax construction below codifies these semantics.

\begin{minted}{syncon}
syntax let:Expression =
  "let" x:Identifier "=" e:Expression
  "in" body:Expression
{
  #bind x in body
  <...>
}
\end{minted}

The syntax type \mintinline{syncon}{Identifier} is a built in type that participates in name binding. \mintinline{syncon}{#bind x in body} specifies that \mintinline{syncon}{x} is part of a definition, otherwise it would be interpreted as a reference that must be bound in the current environment.

\begin{listing}[h]
\begin{minted}{java}
String first = "first";
{  // Opens a new scope
  String second = "second";

  // Error: cannot find third
  System.out.println(first + second + third);
  String third = "third";
  System.out.println(first + second + third);
}  // Closes the new scope

// Error: cannot find second or third
System.out.println(first + second + third);
\end{minted}
\caption{An example in Java demonstrating scopes and imperative style local variables.}
\label{lst:imperative-binding-example}
\end{listing}

Listing~\ref{lst:imperative-binding-example} demonstrates scoping rules in Java. Variables cannot be used before their declaration nor after the scope they were introduced in ends.

These semantics can be described using only capabilities introduced so far, but with some drawbacks. For example, variable declaration, scope introduction and method call could be specified as follows:

\begin{minted}{syncon}
syntax declaration:Statement =
  t:Type x:Identifier "=" e:Expression
  ";" next:Statement
{
  #bind x in next
  <...>
}

syntax scope:Statement =
  "{" s:Statement "}"
  ";" next:Statement
{
  <...>
}

syntax call:Statement =
  e:Expression
  "(" (a:Expression ("," as:Expression)*)? ")"
  ";" next:Statement
{
  <...>
}

syntax empty:Statement = {
  <...>
}
\end{minted}

The idea is to treat each statement as if it contained the following statements as a sub-tree, allowing \mintinline{syncon}{declaration} to bind a name in all following statements. A single ''special'' statement, an empty one, is additionally needed after the final statement.

This specification breaks somewhat with intuition, conceptually Listing~\ref{lst:imperative-binding-example} contains a list of statements, one of which contains sub-statements, but using the specification above it only produces one syntax construction. It also introduces boilerplate, each statement must be written with an extra statement after it, regardless of whether it would normally use it or not, which breaks somewhat with the goal of considering each syntax construction in isolation.

So while bindings in sub-trees are capable of expressing the desired semantics, the description is unintuitive and verbose.

Listing~\ref{lst:javascript-function-binding-example}, however, cannot be expressed using bindings in sub-trees. A function in JavaScript can be used both before and after its declaration, permitting mutual recursion. Binding in sub-tree would then require both functions to be a sub-tree of the other, requiring the abstract syntax tree to be cyclic, which seems undesirable.

\begin{listing}
\begin{minted}{javascript}
function foo(n) {
  console.log("foo", n);
  if (n > 0) bar(n-1);
}
function bar(n) {
  console.log("bar", n);
  if (n > 0) foo(n-1);
}
\end{minted}
\caption{An example in JavaScript demonstrating mutually recursive functions.}
\label{lst:javascript-function-binding-example}
\end{listing}

Instead, a syntax construction is allowed to specify that an identifier is bound before and / or after itself, thus permitting bindings that are not limited to sub-trees. Additionally explicit scopes can be introduced to limit bindings. The syntax construction below has the desired binding semantics.

\begin{minted}{syncon}
syntax funcDecl:Statement =
  "function" f:Identifier "("
  (a:Identifier ("," as:Identifier)*)?
  ")" "{" body:Statement+ "}"
{
  #bind f before
  #bind f after
  #bind f, a, as in body
  #scope (body)
  <...>
}
\end{minted}

This solution once again disconnects statements, allowing each to be defined in isolation, even when they need to affect adjacent statements with a binding.

The interaction between EBNF operators and scopes bears further elaboration. A name used in a \mintinline{syncon}{#scope} declaration refers to all repetitions of the name, i.e., \mintinline{syncon}{#scope (pattern body)} below creates a single scope that encompasses every repetition of \mintinline{syncon}{pattern} and \mintinline{syncon}{body}.

\begin{minted}{syncon}
syntax match:Expression =
  "match" e:Expression "with"
  arm:("|" pattern:Pattern "->" body:Expression)+
{
  #scope (pattern body)
  #scope (e)
  <...>
}
\end{minted}

In this case however we want a scope per arm, as names bound by a \mintinline{syncon}{pattern} should only be available in the corresponding \mintinline{syncon}{body}. Changing the scope declaration to \mintinline{syncon}{#scope arm:(pattern body)} describes exactly that: create one scope per repetition of \mintinline{syncon}{arm} containing only the occurrences of \mintinline{syncon}{pattern} and \mintinline{syncon}{body} within that repetition.

\section{Expansion} \label{sec:design-implementation}

Syntax constructions, being essentially macros, need a way to specify how they expand into whatever underlying language they are implemented on top of. Additionally, the goal of abstraction preservation requires that the implementation of a syntax construction is never exposed to an end user. As such, the expansion must never fail or produce a malformed program.

To achieve this the expansion is specified in a fairly limited way, which makes checking the implementation simpler.

Furthermore, the system requires a base language, some point at which point expansion is finished. Syntax constructions themselves pose no requirements on this language and instead allow a construction to be marked as ''builtin'', meaning part of the base language.

As expansion proceeds and syntax constructions are replaced by syntax constructions closer to the base language a similar transformation will take place on syntax types, a program will go from using the types of its language, to the types of the host language, etc., all the way down to the base language. To ensure that the final expanded program is syntactically correct each syntax type must designate either its underlying type or that it is a syntax type of the base language, i.e. ''builtin''. For example:

\begin{minted}{syncon}
syntax type BaseExpression = builtin

syntax lambda:BaseExpression =
  "fun" x:Identifier "." e:Expression
{
  #bind x in e
  builtin
}
\end{minted}

In the simplest case an expansion is simply a syntactical expression in the target language, with elements of the original syntax construction spliced in, similar to quote and unquote capabilities in similar systems. For example:

\begin{minted}{syncon}
syntax type Expression = BaseExpression

syntax let:Expression =
  "let" x:Identifier "=" e:Expression
  "in" body:Expression
{
  #bind x in body
  #scope (body)
  #scope (e)
  BaseExpression` (fun `id(x). `t(body)) `t(e)
}
\end{minted}

Here a let expression is expanded into a lambda expression (\mintinline{syncon}{fun}) and function application. \mintinline{syncon}{BaseExpression`}, \mintinline{syncon}{`id} and \mintinline{syncon}{`t} assist with disambiguation in parsing, see Section~\ref{sec:disambiguation-implementation} for more details. The underlying type of expressions must agree. It is worth noting here that during expansion all sub-expressions (e.g. \mintinline{syncon}{x} and \mintinline{syncon}{body} above) are treated as atomic, no inspection or modification is permitted or possible.

If the original syntax construction contains EBNF operators some elements may occur multiple times, forming lists of syntactical elements. These may not be used directly in the expansion, where only singular syntactical elements are allowed. To produce a singular syntactical element from a list one may instead use a fold:

\begin{minted}{syncon}
syntax switch:Expression =
  "switch" e:Expression "{"
  cases:("case" test:Expression ":" result:Expression)*
  "default" ":" default:Expression "}"
{
  Expression`
    let x = `t(e) in
    `t(foldr cases rest
         (Expression` if `t(test) == x
                        then `t(result)
                        else `t(rest))
         default)
}
\end{minted}

Here a switch is translated to a simple series of ifs, checking for equality with each case in turn. The fold expression takes four arguments: what list to fold over, the name of the result so far (the accumulator), the expression that will be used to construct the next result, and the initial value.

A reader with previous experience of folds may be somewhat surprised by the syntax used here: nothing looks terribly much like a function and there is no name binding the current item in the list being folded over. Using psuedocode and named arguments the above fold is essentially equivalent with:

\begin{minted}{syncon}
foldr(list = cases,
      initial = default,
      f = ((test, result), rest) =>
        (Expression` if `t(test) == x
                       then `t(result)
                       else `t(rest)))
\end{minted}

The function is run once per repetition of \mintinline{syncon}{cases} and can use all named syntax constructions that are not in a nested inner repetition or in a repetition that does not contain \mintinline{syncon}{cases}. For example, in \mintinline{syncon}{example} below the following things are true:
\begin{itemize}
  \item The first two folds are equivalent, since \mintinline{syncon}{second} and \mintinline{syncon}{a} repeat exactly as often.
  \item All bodies can use \mintinline{syncon}{a} as a singular value.
  \item Only \mintinline{syncon}{body3} and \mintinline{syncon}{body4} can use \mintinline{syncon}{b} as a singular value.
  \item The third fold will use \mintinline{syncon}{body3} once per occurrence of \mintinline{syncon}{b} across all repetitions of \mintinline{syncon}{second}, while the nested fold in the fourth will only use \mintinline{syncon}{body4} once per \mintinline{syncon}{b} in the \emph{current} repetition of \mintinline{syncon}{second}.
\end{itemize}

\begin{minted}{syncon}
syntax example:T =
  first:"first"+ second:(a:T b:T*)+
{
  foldr second acc
    <body1>
    <initial>

  foldr a acc
    <body2>
    <initial>

  foldr b acc
    <body3>
    <initial>

  foldr second acc1
    (foldr b acc2
      <body4>
      <initial>)
    <initial>
}
\end{minted}

Having all named syntax elements implicitly available turned out to be more convenient than requiring some form of pattern matching or explicitly mentioning the ones you wanted to use, and presents no ambiguity for the reader as long as you already know the syntax.

The example uses a right fold, additionally left fold and versions without an initial value are available, the latter only being usable if the list contains at least one value (i.e. it was constructed using the ''+'' operator, not ''*'' or ''?'').

\subsection{Expansion Checking and Correctness} \label{sec:expansion-checking}

One of the design goals of syntax constructions is that they must provide good error messages when some aspect of the system is misused. For the purposes of this section a good error message presents a user with the information required to fix the error, and presents that information in terms of code that user is working with. Notably then a poorly implemented syntax construction should result in an error message for the syntax construction implementer, not for an unsuspecting end-user using the syntax construction. Similarly, an end-user misusing a syntax construction should get an error message referring to their code, not to its expansion.

To ensure that this split is maintained we require that expansion never introduces an error, i.e. error-free code is expanded into error-free code. This places a fairly stringent requirement on the syntax construction implementer: the expansion of a syntax construction must never introduce an error, no matter what. The meaning of error-free bears explaining however. Syntax constructions in their current incarnation, following the delimitations set in Section~\ref{sec:delimitations}, know only one aspect of the semantics of its languages: name binding. ''Error-free'' thus means ''without binding errors'', e.g. no unbound references or duplicate definitions.

Presenting an error to an end-user misusing a syntax construction is then straight-forward: perform name resolution on the unexpanded abstract syntax tree and report any error encountered. If there are no errors the code is correct and can be fully expanded without introducing errors.

Ensuring that an expansion will never introduce an error is rather trickier. The expansion must be correct regardless of the structure of the surrounding syntax tree. For example, the following definition of \mintinline{syncon}{let}, that translates a \mintinline{syncon}{let}-expression to an application of a lambda, contains a somewhat subtle bug:

\begin{minted}{syncon}
syntax let:Expression =
  "let" x:Identifier "="
  e:Expression "in"
  body:Expression
{
  #bind x in body
  #scope (body)
  Expression`
    (fun `id(x). `t(body)) `t(e)
}
\end{minted}

If the syntax construction represented by \mintinline{syncon}{e} binds a name, for example \mintinline{syncon}{foo}, using \mintinline{syncon}{#bind foo after}, and the \mintinline{syncon}{body} uses it, then this expansion will introduce an error. The expansion has \mintinline{syncon}{e} appearing \emph{after} \mintinline{syncon}{body}, which breaks the reference and introduces an error, namely that \mintinline{syncon}{foo} in \mintinline{syncon}{body} is unbound.

The language designer may at this point consider this somewhat nonsensical, perhaps no expressions expose names in that way, but since syntax constructions are made for composition and reuse we cannot assume that to always be true. Thus checking assumes an open universe of syntax constructions, i.e. all syntax types conceptually include syntax constructions that both export and use names in all possible ways, and an expansion must not introduce an error, even in their presence.

In this particular case the solution is to place \mintinline{syncon}{e} in its own scope, which prevents \mintinline{syncon}{body} from referencing any names from \mintinline{syncon}{e}. Additionally this aligns better with the intended semantics; it specifies explicitly that nothing bound in \mintinline{syncon}{e} can be used elsewhere.

Another common aspect of rewriting systems is that of name capture, most commonly unintentional name capture. The expansion of a syntax construction cannot perform name capture, intentional or not, by construction. The guarantee that is given is that if an identifier $id_r$ references a binding identifier $id_b$ before expansion then any copy of $id_r$ present after expansion references some copy of $id_b$. In combination with the original syntax tree not containing any unbound references (since they would produce an error) this prevents any form of name capture.

This preservation of reference is done by ensuring that each symbol present in the syntax tree only occurs in binding position once. This means that if $id_r$ is bound after expansion, then it must be bound to a copy if $id_b$, since it is the only identifier with that symbol. Thus absence of errors implies absence of name capture.

Binding symbols are kept unique by having name resolution replace each symbol with a newly generated, unique symbol in both binding and referencing positions. Since an expansion may use a syntactical element twice or more each expansion may need a new name resolution pass. This pass is guaranteed to succeed, as per the guarantees mentioned above, and is only necessary to generate new symbols.


\subsection{Disambiguation in Expansion Parsing} \label{sec:disambiguation-implementation}

The expansion specification of a syntax construction is intended to be written in some underlying language as a sort of template, into which the child nodes of the syntax construction can be slotted to produce the expanded abstract syntax tree. Parsing this template turns out to be ambiguous without certain extra precautions, which are motivated and described in the next two sections.

\subsubsection{Template Type Annotations}

As mentioned previously the underlying types must agree throughout the expansion, for example the expanded syntax tree must have the same underlying type as the syntax construction itself; i.e. the expansion uses structural typing.

Thus, when parsing the expansion specification any syntax construction that shares an underlying syntax type is valid. However, since the core language is likely to be simple, but flexible, it is likely to have few syntax types. For example, the core language used during evaluation (Section~\ref{sec:evaluation}) contains a single syntax type: \mintinline{syncon}{BExpression}. As such, many non-core languages will have syntax types that share an underlying type, which becomes a problem if their concrete syntax also overlaps.

As an example, consider the code ''\mintinline{ocaml}{[1]}'' in OCaml. Depending on context it may be parsed as a list literal, evaluating to a list with the single element \mintinline{ocaml}{1}, or as a pattern, matching a list containing \mintinline{ocaml}{1} as its only element. When implementing these constructions using syntax constructions they would have different syntax types, \mintinline{syncon}{Expression} and \mintinline{syncon}{Pattern}, respectively. Their underlying type would however be shared, assuming they use the aforementioned core language. With this in mind, the following syntax construction does not parse unambiguously:

\begin{minted}{syncon}
syntax oneList:Expression = "one" {
  [1]
}
\end{minted}

To solve this problem an explicit type annotation is required:

\begin{minted}{syncon}
syntax oneList:Expression = "one" {
  Expression` [1]
}
\end{minted}

\subsubsection{Splicing Child Nodes}

Using child nodes in the implementation, splicing them into the template, also requires a form of type annotation, but this comes more out of a limitation of the current implementation than of a need for the general method.

Consider the implementation of the syntax construction \mintinline{syncon}{postfixAdd} below, assuming the three preceding syntax construction are in scope and available for use.

\begin{minted}{syncon}
syntax add:Expression =
  a:Expression "+" b:Expression
{ builtin }

syntax variable:Expression =
  id:Identifier
{ builtin }

syntax literal:Expression =
  int:Integer
{ builtin }

syntax postfixAdd:Expression =
  a:Expression b:Expression "+"
{
  Expression` a + b
}
\end{minted}

The appearances of \mintinline{syncon}{a} and \mintinline{syncon}{b} in the body are meant to refer to the child nodes declared in the syntactic description. When parsing the body they must thus be parsed as unknown syntax constructions of type \mintinline{syncon}{Expression}, even though they syntactically appear as identifiers.

However, the parser in the current implementation is based entirely on context free grammars and can thus not use the information from the syntax description when parsing the body (that would be context-\emph{sensitive}). Parsing the above syntax construction with a context free parser would require trying to interpret each identifier either as an identifier, or a spliced child node, and then disambiguating afterwards.

This turns out to create a massively ambiguous grammar. In the example above the identifier \mintinline{syncon}{a} could be interpreted as one of four things:
\begin{enumerate}
  \item A normal identifier, thus parsing as a \mintinline{syncon}{variable} syntax construction containing an identifier with the symbol ''a''.
  \item A spliced identifier, thus parsing as a \mintinline{syncon}{variable} syntax construction containing an identifier with an unknown symbol.
  \item A spliced integer, thus parsing as a \mintinline{syncon}{literal} syntax construction containing an integer with an unknown value.
  \item A spliced \mintinline{syncon}{Expression}.
\end{enumerate}

The first two cases could plausibly be handled in the same way, reducing it to three valid parses. The total number of parse trees produced with this grammar is nonetheless still exponential in the number of identifiers in the source. Introducing syntax for splicing disambiguates the first case from the remaining three, and requiring a limited form of type tagging disambiguates between the remaining three.

The solution used in this thesis requires tagging to distinguish a spliced identifier (\mintinline{syncon}{`id}), integer (\mintinline{syncon}{`int}), float (\mintinline{syncon}{`float}), string (\mintinline{syncon}{`str}), or syntax construction (\mintinline{syncon}{`t}). Tagging a splice as a syntax construction, rather than a specific syntax type, appears sufficient. No ambiguous implementations have appeared in practice thus far, though they are still possible.

\chapter{Formalization} \label{sec:formalization}

The definition of a syntax construction presented herein is given in a top-down order, starting with the syntax construction itself, then filling in the necessary details one at a time. Paths in particular are explained in their own section, Section~\ref{sec:paths}, but briefly, they select some subset of the children of a syntax construction or syntax construction instance.

Some parts of this formalization will use a line above an expression to mean a sequence of expressions. For example, $\overline{i}$ denotes a sequence of $i$s, $i_n$ denotes the $n$th element in the sequence, and $\overline{i_n + 1}$ denotes a sequence of $i_n + 1$ for each $n$ (i.e. a mapping over the sequence). Given two sequences $\overline{a}$ and $\overline{b}$, $\overline{(\overline{a}, b_n)}$ is a sequence where each $b_n$ is paired with the entire sequence $\overline{a}$.

A syntax construction $c$ is given by a tuple
$$ c = (t_c, syntax, \overline{scope}, B_i, B_b, B_a, p_c, a_c, e_c) $$
with the following components:

\begin{tabular}{r|p{10cm}}
$t_c$ & The syntax type of $c$ \\
$syntax$ & The syntax description of $c$ \\
$\overline{scope}$ & The scopes introduced by $c$ \\
$B_i$ & The nested bindings of $c$, a set of tuples $(path, \overline{path})$ where the first element is a path to an identifier and the second is paths to where it is in scope. \\
$B_b$ and $B_a$ & The bindings $c$ exports before and after itself, respectively. Sets of paths to identifiers. \\
$p_c$ & The precedence of $c$, $+\infty$ unless otherwise specified. \\
$a_c$ & The associativity of $c$. \\
$e_c$ & The expansion of $c$. \\
\end{tabular}

The structure of $syntax$ is given by the following grammar:

\setlength{\grammarindent}{6em}
\begin{grammar}
<syntax> ::= 'Identifier'
  \alt 'Symbol'
  \alt 'String'
  \alt 'Integer'
  \alt 'Float'
  \alt <Token-Literal>
  \alt t
  \alt '(' <syntax>* ')'
  \alt <syntax> '*'
  \alt <syntax> '+'
  \alt <syntax> '?'
\end{grammar}

From here we define the concept of a leaf of a syntax construction, those descendants that appear atomic: the first seven alternatives in the grammar above are leaves.

When a syntax construction $c$ is parsed we create an \emph{instance} of it, which is defined by the following grammar:

\setlength{\grammarindent}{6.5em}
\begin{grammar}
<Instance> ::= c <Inner>

<Inner> ::= <Token>
  \alt <Instance>
  \alt '(' <Inner>* ')'
  \alt '[' <Inner>* ']'
\end{grammar}

The structure of an instance has a very direct mapping to the syntax description of the construction itself; the first six alternatives of the description map to a \synt{Token}, $t$ maps to an \synt{Instance}, a parenthesized sequence maps to a parenthesized sequence, and an EBNF operator maps to a bracketed sequence. It is worth noting here that for our purposes an instance and an abstract syntax tree are the same thing.

A scope $s$ is given by a tuple

$$ s = (path, \overline{path}, scope) $$

where the first component describes how to repeat the scope in the presence of EBNF operators, the second gives paths to the leaves that are part of the scope, and the third is the parent scope. Additionally there is a special scope $near = ([], \overline{unscoped}, \bot)$, to represent the scope in which an instance of $c$ resides. No two scopes should share any paths in the second element, i.e. all leaves should belong to exactly one scope, and $\overline{unscoped}$ should contain all leaves that are not covered by some other scope, i.e. all leaves belong to a scope. Additionally, the path in the first component must be a prefix of all paths in the second component.

An expansion $e_c$ is given by the following grammar:

\setlength{\grammarindent}{8em}
\begin{grammar}
<Expansion> ::= $path$
  \alt 'acc' $n$
  \alt 'foldl' $path$ <Expansion> <Expansion>
  \alt <Template>

<Template> ::= c <Inner>

<Inner> ::= <Token>
  \alt <Expansion>
  \alt '(' <Inner>* ')'
  \alt '[' <Inner>* ']'
\end{grammar}

It is essentially an instance, except with the capability of splicing in an element from another instance, or folding over many of them, to create a new instance of some other construction. The 'acc' $n$ alternative refers to the accumulator of a surrounding fold, the innermost if $n = 0$, second inner-most if $n = 1$, etc., essentially a de bruijn index. The actual implementation additionally has a right fold, as well as $fold1$ variations of both, where the first element of the list is used as the initial value of the accumulator while folding over the tail of the list, but these are omitted here for brevity.

An additional point here is that of recursion: many macro systems allow a macro to expand to code containing the macro itself, but syntax constructions do not (neither directly nor indirectly). Disallowing recursion loses some expressive power, but EBNF operators and folds mostly make up for this loss in practice, and we gain some nice termination guarantees instead.

\section{Source Code Parser}

Parsing a language (i.e. a set of syntax constructions) assumes the presence of an algorithm to parse a context free grammar and give all possible parse trees. If there is a single parse tree the parse was successful, if there are none it was not, and if there are many the parse was ambiguous.

The algorithm for translating a set of syntax constructions to a context free grammar proceeds largely as described below, using the following notation:

\mathligsoff
{\tabulinesep=2mm
\begin{tabu}{c|p{10cm}}
$N^t_p$ & A non-terminal generated by the constructions $c$ such that $t = t_c \land p = p_c$. \\
$next(N^t_p)$ & The non-terminal with the next higher precedence. \\
$N^t_-$ & Shorthand for the non-terminal generated by the lowest precedence group in $t$. \\
$N^t_+$ & Highest precedence group in $t$. Note that $N^t_+ \neq N^t_-$ is not true in general, some syntax types only contain syntax constructions of a single precedence level. \\
\end{tabu}
}

\begin{enumerate}
  \item Group syntax constructions by their syntax type, then by precedence.
  \item For each such group, generate a non-terminal $N^t_p$:
  \begin{enumerate}
    \item For each syntax construction $c$, generate one production according to its syntax description:
    \begin{itemize}
      \item Identifier, integer, float, string, and symbol each match one token of the corresponding kind.
      \begin{itemize}
        \item Identifier should not match an identifier with a symbol that is used as a literal in any syntax construction.
      \end{itemize}
      \item A token literal matches a token of exactly the same form.
      \item References to a syntax type $t'$ matches non-terminals according to the following, picking the first that applies:
      \begin{itemize}
        \item If $t \neq t'$, match $N^{t'}_-$.
        \item If $c$ has the highest precedence in $t$, match $N^t_-$.
        \item If $c$ has undefined associativity, or is left-associative and $t'$ is the leftmost occurrence, or is right-associative and $t'$ is the rightmost occurrence, match $N^t_{p_c}$.
        \item Otherwise match $next(N^t_{p_c})$.
      \end{itemize}
      \item A parenthesized sequence matches each of its children in turn.
      \item EBNF operators match repetitions as expected.
    \end{itemize}
  \end{enumerate}
  \item For each non-terminal $N^t_i$ except $N^t_+$ add a production matching the non-terminal $next(N^t_i)$.
\end{enumerate}
\mathligson

\subsection{Ambiguity Detection} \label{sec:implementation-ambiguity-detection}

If the generated context free grammar is ambiguous and we parse source code that exposes this ambiguity, i.e., the source code can be parsed in multiple ways, we wish to produce a useful error message. The means by which we construct this error message is technically not needed to implement the correct semantics of syntax constructions, but given the presence of a design goal relating solely to the quality of error messages it seemed prudent to include it.

In such a case the parser produces multiple abstract syntax trees, i.e., detecting that there is an ambiguity is trivial, but the mere fact that a file can be parsed in multiple ways provides no actionable information towards fixing the problem.

Instead we wish to produce an error that refers only to the actually ambiguous parts of the source file. Furthermore, we wish to present the different parsings in an understandable way.

Starting with the second point, assume that no operators have defined precedence or associativity and consider the following expression:

$$ a + b * c $$

It can be parsed in two ways:

$$ (a + b) * c $$
$$ a + (b * c) $$

The former is a product of a sum and a variable, the latter is a sum of a variable and a product. Further describing the inner structure of the sub-expressions (sum of two variables and product of two variables respectively), gives little to no extra useful information for understanding the two parsings. Consider this more complicated example:

$$ a + b + c + d $$

The parsings are as follows:

$$ ((a + b) + c) + d $$
$$ (a + (b + c)) + d $$
$$ (a + b) + (c + d) $$
$$ a + ((b + c) + d) $$
$$ a + (b + (c + d)) $$

All of these are sums, the first two of a sum and a variable, the next of two sums, the last two of a variable and a sum. It is the hypothesis of this thesis that this description is sufficient, i.e. each parsing can be described by a single top-level node (here sum) and a shallow description of its children (here sum and variable, two sums, and variable and sum respectively). Note that each of these would also include the area of the source they cover as well. This will be referred to as a two-level representation. It appears to be enough information for a user to solve one part of the ambiguity without being overwhelmed.

The task of producing an error message then becomes selecting sub-trees from the syntax trees such that:

\begin{itemize}
  \item The unselected parts are identical across the different syntax trees, including covered source area (i.e. we report all ambiguities).
  \item The selected sub-trees from the same position across the syntax trees cover the same source area (i.e. each report has a clear connection to the source).
  \item At least two selected sub-trees in the same position of their respective syntax tree have different two-level representations (i.e. all reports actually show different parsings).
  \item No selected sub-tree is contained in another (i.e. we only report one ambiguity per source file area).
  \item No selected sub-tree can be replaced by one of its children (also replacing the corresponding sub-trees from the other syntax trees) while still satisfying the points above (i.e. the selected sub-trees are minimal).
\end{itemize}

The algorithm is fairly simple. We use a form of shallow equality where two nodes are equal if they cover the same source area and have the same kind (e.g. they are instances of the same syntax construction). With this in mind we traverse all the syntax trees in parallel, top to bottom, until we encounter a node that is shallowly different between at least two syntax trees. If any sibling of the node also is shallowly different, select the parent, otherwise select the node and keep traversing down the siblings.

Finally, present to the user a set of two-level representations for each selected sub-tree across the syntax trees.

% TODO: it is not actually obvious that the algorithm satisfies all the requirements above, nor that all requirements are required, nor that it's always possible to satisfy all requirements.

% it satisfies covers same area (because difference in area would mean start and end point are different, i.e. one of the versions covers a token the other does not, but then that token must be covered by a sibling, or a sibling of the parent, etc., in which case the sibling is shallowly different, or the parent is shallowly different, etc.)
% it satisfies no overlap (trivial)
% it satisfies minimal (only select something shallowly equal if multiple children are shallowly different, i.e. would need to replace by multiple sub-trees)
% it satisfies shows difference (trivial)
% it satisfies report all (trivial)

% TODO: the minimal requirement is a bit weirdly formulated, we might rephrase it as (no [...] can be replaced by one or more of its children [...]), but then we need to check especially for coverage differences, so if any of the children have coverage differences across the ASTs then we select the parent, otherwise we select the children

\section{Paths} \label{sec:paths}

A $path$ is a sequence of steps directing traversal through a tree, either the $syntax$ description of a syntax construction or the actual syntax tree of an instance. Each step is either a number $n$, traversing down the $n$th child of the current node, or a $*$ traversing down all children of the current node. We use $[]$ for the empty path and $n:path$ for prepending a step $n$ to path $path$.

All paths belong to a single syntax construction and describe a path from it to one or more of its descendants, no paths traverse through a child syntax construction or instance.

Furthermore, all paths that are part of the definition of $c$ are limited in the following way: they select either
\begin{itemize}
  \item one \synt{Syntax} element in $syntax$, or
  \item zero or more \synt{Inner} elements of an instance of $c$.
\end{itemize}
Both cases are written as function application, i.e. $path(syntax)$ selects a \synt{Syntax} element and $path(c~i)$ selects a set of \synt{Inner} elements. More concretely:

$$
\begin{array}{rcl}
([])(syntax) & = & i \\
(n : path)((\overline{syntax})) & = & path(syntax_n) \\
(\_ : path)(syntax~*) & = & path(syntax) \\
(\_ : path)(syntax~+) & = & path(syntax) \\
(\_ : path)(syntax~?) & = & path(syntax) \\
\end{array}
$$

and

$$
\begin{array}{rcl}
([])(i) & = & \{i\} \\
(n : path)((\overline{syntax})) & = & path(syntax_n) \\
(n : path)([\overline{syntax}]) & = & path(syntax_n) \\
(* : path)([\overline{syntax}]) & = & \bigcup_n path(syntax_n) \\
\end{array}
$$

Additionally we define a relation $p_1 \subseteq p_2$ between two paths, where

$$
\begin{array}{ll}
path \subseteq path & \\
n : path \subseteq n : path' & \text{ if } path \subseteq path' \\
n : path \subseteq * : path' & \text{ if } path \subseteq path' \\
\end{array}
$$

% TODO: the \forall here is a little bit iffy, since we only consider things for a single syntax construction
We justify the notation by noting that if $p_1 \subseteq p_2$ then $p_1(syntax) = p_2(syntax)$ and $\forall c~i.\ p_1(c~i) \subseteq p_2(c~i)$ (keeping in mind that a path is limited to the syntax construction in which it is defined, that it must select a single \synt{Syntax} element in $syntax$, and that the structure of an instance mirrors the structure of $syntax$). We say that $p_1$ is a more specific version of $p_2$.

We also wish to be able to specialize a $path$ using some more specific path $spec$, i.e. $specialize(path, spec) \subseteq path$. Note that the fourth case below allow $path$ and $spec$ to diverge (i.e. we do not require $spec \subseteq path$) in which case the specialization is only applied to their shared prefix.

$$
\begin{array}{rcl}
specialize(path, []) & = & path \\
specialize(*:path, n:spec) & = & n : specialize(path, spec) \\
specialize(*:path, \_:spec) & = & * : specialize(path, spec) \\
specialize(n:path, n':spec) & = &
\begin{cases}
  path & n \neq n' \\
  n : specialize(path, spec) & n = n' \\
\end{cases} \\
\end{array}
$$

\section{Expansion} \label{sec:expansion-formalization}

Expanding the instance of a syntax construction is a relatively straightforward affair, but the formalization turns out to be surprisingly involved. The base idea is that an expansion is a template into which we splice sub-trees from the instance we are expanding, similar to pattern based macros in Lisp tradition.

The process is made somewhat more complicated by the presence of EBNF operators that generate lists, which must be reduced to a single sub-tree through folds. To perform such a fold we require specific paths to each of the elements to be folded over, to be handed to $specialize$ later:

$$
\begin{array}{rcl}
specs([], i) & = & \{[]\} \\
specs(n:path, (\overline{i})) & = & \{n : p \mid p \in specs(path, i_n)\} \\
specs(n:path, [\overline{i}]) & = & \{n : p \mid p \in specs(path, i_n)\} \\
specs(*:path, [\overline{i}]) & = & \bigcup\limits_{n} \{n:p \mid p \in specs(path, i_n)\} \\
\end{array}
$$

Note that while this function uses set-notation, the order of the paths will be important while folding. The order is easily recovered however using a simple lexicographic sorting of the paths.

Expansion (via $expand$ below) then traverses the template (the fourth argument), while keeping track of the current item in a fold ($spec$), the related accumulators ($\overline{acc}$), and the instance being expanded ($inst$). Most cases are fairly straightforward:

$$
\begin{array}{rcl}
expand(inst, spec, \overline{acc}, path) & = & i \text{ where } path(inst) = \{i\} \\
expand(inst, spec, \overline{acc}, \text{acc}~n) & = & acc_n \\
expand(inst, spec, \overline{acc}, c~i) & = & c~expand(inst, spec, \overline{acc}, i) \\
expand(inst, spec, \overline{acc}, token) & = & token \\
expand(inst, spec, \overline{acc}, (\overline{i})) & = & (\overline{expand(inst, spec, \overline{acc}, i_n)}) \\
expand(inst, spec, \overline{acc}, [\overline{i}]) & = & [\overline{expand(inst, spec, \overline{acc}, i_n)}] \\
\end{array}
$$
The one remaining case, $foldl$, is somewhat more involved:
$$
\begin{array}{rcl}
expand(inst, spec, \overline{acc}, \text{foldl}~path~f~i) & = & foldl(f', i', specs') \\
\end{array}
$$
where
$$
\begin{array}{rcl}
f' & = & \lambda spec'~acc'.~ expand(inst, spec', acc'~\overline{acc}, f) \\
i' & = & expand(inst, spec, \overline{acc}, i) \\
specs' & = & specs(specialize(path, spec), inst) \\
\end{array}
$$

and $foldl$ is a regular left fold over a list.

% TODO: this is not the most understandable justification, to say the least. there is a part of an alternate explanation below it
At this point we can consider termination guarantees: the expansion of a single syntax construction instance must terminate. We see this easily from the definition of $expand$ and the observation that the original syntax tree is finite. Furthermore, since syntax constructions may not recursively expand into instances of themselves (neither directly nor indirectly), the complete expansion of a syntax tree must also terminate. We can see this by noting that when we expand an instance of a syntax construction $c$ then none of the instances introduced by the expansion (i.e. not the ones copied over from the previous instance) are instances of $c$, nor can they expand into instances of $c$. To expand those instances we thus never need $c$, i.e., the number of syntax constructions required for expansion is reduced by one. Since the number of syntax construction is finite we will eventually reach a point where we cannot expand any further since no remaining syntax construction has an expansion, i.e., the syntax tree is fully expanded.

% We can see this by forming a directed acyclic graph where there is an edge from a syntax construction to another if the first might expand to a syntax tree containing the other, then sorting the syntax constructions in topological order. If we then expand all occurrences of the first syntax construction (call it $c$) the resulting syntax tree contains no instances of $c$, nor can an instance of $c$ occur from further expansion of other syntax construction. Expanding the second syntax construction similarly removes it, etc., until

\section{Name Resolution}

Name resolution is a structure and binding preserving transformation that replaces each symbol in an abstract syntax tree with newly generated ones so as to ensure that each symbol is only bound once across the entire tree. The root of the tree will be referred to as $root$, and will by construction have the shape $c~i$ (i.e., it is an instance of some syntax construction $c$).

We assume that each node is unique and distinct from all other nodes in the tree, and in particular that it can be used as the key in a mapping. This could, for example, be accomplished by attaching a unique number to each node across the tree. Furthermore, we can attach an ordering between two nodes if neither is a descendant of the other using their position in the syntax tree; a node $a$ is before another node $b$ if it appears further to the left in the tree, in which case we will write $a < b$.

% TODO: is "to the left" sufficiently well defined?

$$
\begin{array}{rcl}
leaves(c~i) & = & children(i) \\
children(token) & = & \{token\} \\
children(c~i) & = & \{c~i\} \\
children((\overline{i})) & = & \bigcup\limits_n children(i_n) \\
children([\overline{i}]) & = & \bigcup\limits_n children(i_n) \\
\end{array}
$$

$$
\begin{array}{rcl}
binders(c~i) & = & i_b \cup i_a \cup i_i \cup i_c \\
\text{where} & & \\
i_b & = & \{ path(c~i) \mid path \in B_b \text{ of } c\} \\
i_a & = & \{ path(c~i) \mid path \in B_a \text{ of } c\} \\
i_i & = & \{ path(c~i) \mid (path, \_) \in B_i \text{ of } c\} \\
i_c & = & \bigcup_{c'~i' \in leaves(c~i)} binders(c'~i') \\
\end{array}
$$

Define a mapping $\Gamma$ from $binders(root)$ to new unique symbols.

Second we wish to determine the scope of each node in the tree. Similar to syntax constructions and their instances we introduce scope instances, where a scope instance $s'$ that is an instance of scope $s$ is given by

$$ s' = (path, s, parent) $$

where $parent$ is another scope instance, and the first component is the same path as in the scope except every $*$ is replaced by some $n$. Additionally we introduce a special scope instance $s'_r$ representing the root scope. We will further write $s'_c -> s'_p$ to mean $s'_p$ is the parent scope of $s'_c$, and $s' ->* s''$ as the reflexive, transitive closure of this relation. Since each node will be assigned to a single scope we will write $i \in s'$ to mean that $i$ belongs to scope instance $s'$, which then selects a unique scope instance per $i$.

$$ root \in s_r $$

% TODO: need to discuss difference between symbol and identifier, probably in the first section

$$
\begin{array}{rcl}
instantiate(c~i, path, (rep, \_, s)) & = & (specialize(rep, path), instantiate(c~i, path, s)) \\
instantiate(c~i, path, near) & = & s' \text{ where } c~i \in s' \\
\end{array}
$$

$$
\inference{
  i \in leaves(c~i_p) &
  path(c~i_p) = \{i\} &
  path \subseteq path' \\
  path' \in paths &
  (\_, paths, \_) = s \in scopes \text{ of } c
}{i \in instantiate(c~i_p, path, s)}
$$

We can now start considering actual bindings. For each node we will compute a set of available bindings, where each binding is represented by a tuple $(sym, s', gensym)$. The first two components denote that $sym$ is defined in scope $s'$, while the last component is the newly generated symbol (from $\Gamma$ above) that will replace it.

The $lookup$ function below defines shadowing behavior: a binding of a symbol in a child scope shadows the same symbol in a parent scope. The first case also assumes that we will never produce a binding set with more than one element that share the first two components, which would represent a redefinition of a symbol in the same scope.

$$
lookup(sym, s', \gamma) =
\begin{cases}
gensym & (sym, s', gensym) \in \gamma \\
lookup(sym, s'', \gamma) & (sym, s', \_) \not\in \gamma \land s' -> s'' \\
\bot & (sym, s', \_) \not\in \gamma \land s' = s_r
\end{cases}
$$

At this point we have our definitions for binding errors: an unbound symbol produces $\bot$ during lookup, while a redefinition produces a binding set where two or more elements share the first two components.

The latter error has some subtleties. We could defer the error until $lookup$ would find multiple choices, i.e. make it an error to have an ambiguous reference instead of making it an error to redefine a symbol, but the latter tends to be a programming error even if it does not lead to any ambiguity, so it was chosen instead.

The exported ''before'' and ''after'' bindings of an instance $c~i \in s'$, written as $E_b(c~i)$ and $E_a(c~i)$ respectively, are given by:

$$
\begin{array}{rcl}
E_b(c~i) & = & immediate \cup transitive \\
\text{where} & & \\
transitive & = & \bigcup_{c'~i' \in leaves(c~i), c'~i' \in s'} E_b(c'~i') \\
immediate & = & \bigcup_{path \in B_b} \{(symbol(path(c~i)), s', \Gamma(path(c~i)))\}
\end{array}
$$

$$
\begin{array}{rcl}
E_a(c~i) & = & immediate \cup transitive \\
\text{where} & & \\
transitive & = & \bigcup_{c'~i' \in leaves(c~i), c'~i' \in s'} E_a(c'~i') \\
immediate & = & \bigcup_{path \in B_a} \{(symbol(path(c~i)), s', \Gamma(path(c~i)))\}
\end{array}
$$

Bindings can also be defined in a nested manner, via $B_i$. The bindings visible in a node $i \in leaves(c~i_p)$ where $path(c~i_p) = \{i\}$ is given by $E_i(i)$:

% TODO: there are some rather complicated things we range over in some \bigcup(s), need a better way to write that

$$
\bigcup_{p_i, path \subseteq path', path' \in p, (p_i, p) \in B_i \text{ of } c} \{(symbol(p(c~i_p)), s', \Gamma(p(c~i)))\} \text{ where } p(c~i_p) \in s'
$$
$$
above \cup E_i(c~i_p)
$$

With all this setup, name resolution is a matter of replacing the symbol in each identifier that was parsed from 'Identifier' with a newly generated symbol. Given an identifier $ident \in leaves(c~i)$ with scope $ident \in s'$, $path(c~i) = \{ident\}$, and $syntax$ being the syntax description of $c$, we will only perform the substitution if $path(syntax) = \text{'Identifier'}$.

$$
\begin{array}{rcl}
symbol(ident) & <- &
\begin{cases}
\Gamma(ident) & ident \in binders(root) \\
lookup(symbol(ident), s', \gamma) & ident \not\in binders(root) \\
\end{cases} \\
\text{where} & & \\
\gamma & = & a \cup b \cup E_i(ident) \\
a & = & \bigcup_{c'~i' \in s'' \land s' ->* s'' \land c'~i' < ident} E_a(c'~i') \\
b & = & \bigcup_{c'~i' \in s'' \land s' ->* s'' \land c'~i' > ident} E_b(c'~i') \\
\end{array}
$$

\section{Expansion Checking} \label{sec:expansion-checking-formalization}

Expansion checking extends name resolution to handle special 'placeholder' nodes, nodes that represent instances of syntax constructions, and that conceptually reference all symbols in scope and export one symbol \mintinline{syncon}{before}, one \mintinline{syncon}{after}, and one in both.

By definition, for a placeholder node $p \in s'$ we have

$$ binders(p) = \{p_e, p_b, p_a\} $$
$$ E_b(p) = \{(p_e, s', \Gamma(p_e)), (p_b, s', \Gamma(p_b))\} $$
$$ E_a(p) = \{(p_e, s', \Gamma(p_e)), (p_a, s', \Gamma(p_a))\} $$

Additionally we will consider the computed set of bindings in scope (i.e. $\gamma$) for a placeholder node, but without the generated symbols, to be an output of name resolution, which we will refer to as $\gamma_p$.

The idea here is that no actual instance will have greater requirements on its surroundings than a placeholder node, thus if expanding an instance where all children are placeholders introduces no errors, then expanding the same instance but with actual instances should also not introduce errors.

More concretely, given a syntax construction instance $c~i$ where all identifiers have different symbols and all child instances are in fact placeholders, and its expansion $e$, then an error might be introduced if one of the following is true:

\begin{enumerate}
  \item There is a placeholder $p'$ somewhere in $e$, copied from $p$ in $c~i$, such that $\gamma_p \setminus \gamma_{p'} \neq \emptyset$ (i.e. $p$ might reference a symbol before expansion that $p'$ cannot afterwards).
  \item $E_b(e) \setminus E_b(c~i) \neq \emptyset$ or $E_a(e) \setminus E_a(c~i) \neq \emptyset$ (i.e. the instance might export something before expansion, but not after).
  \item Any symbol in a referencing identifier in $e$ that is unbound also appears unbound in a referencing identifier in $c~i$.
\end{enumerate}

The third point warrants further elaboration. The checking procedure examines an instance in isolation, i.e. it is not a sub-tree of some syntax tree. This means that any identifier that appears in a referencing position is unbound, which is an error. However, if the same instance appeared in a syntax tree and that tree had no binding errors, then all those identifiers must be bound in that context. Swapping the instance for its expansion and placing it in the same context then satisfies the same references in the expansion. A new unbound error with a new symbol would not have this guarantee and might thus introduce an error where there were none before.

Checking the expansion definition of a syntax construction then becomes a matter of generating a representative set of instances with placeholders that cover all possible real instances that could appear in a real syntax tree. In the absence of EBNF operators this is a single instance, since no variation is possible without picking different child instances, but in their presence more care must be taken.

A 'zero or one' repetition ($?$) clearly requires two variations, but the unbounded repetitions ($*$ and $+$) are less clear.

A fold over a '$*$' repetition could result in the initial accumulator value only, or one or more iterations of the actual folding template. The folding template can only use the accumulator atomically, so it will essentially wrap it in some other expression, possibly copied in multiple places. It is the hypothesis of this thesis that errors can occur either when fold and initial value interact, or when a fold interacts with a previous fold, and that no such interaction requires multiple folds to appear, because of the limited capabilities of each fold. If this holds true then all possible errors should appear by simply checking 0, 1, and 2 repetitions. Similarly, a '$+$' repetition could be used in a $fold1$ fold which would require 3 repetitions to exhibit two wrapped applications of the fold template.

The $gen$ function, given below, generates the necessary variations. Note that it is not written as a pure function, the first two cases should generate new unique results every time they are called. Also note that the third case (sequences) and $rep$ should be considered to generate one variation per choice of variation from the sets generated by their children, basically a set cross-product. The nodes of a syntax description that do not interact with name binding can be largely ignored, except that something must be in their place in the instance, otherwise paths would not lead to the correct node in the instance.

% TODO: is case three (sequence) clear in what it means? (all possible combinations choosing one from the variations of each item in the sequence) (essentially crossproduct of all the respective sets)

$$
\begin{array}{rcl}
gen(\text{'Identifier'}) & = & uniqueId \\
gen(t) & = & uniquePlaceholder \\
gen((\overline{syntax})) & = & \{(\overline{gen(syntax_n)})\} \\
gen(syntax~*) & = & \bigcup_{n=0}^2 rep(syntax, n) \\
gen(syntax~+) & = & \bigcup_{n=1}^3 rep(syntax, n) \\
gen(syntax~?) & = & \bigcup_{n=0}^1 rep(syntax, n) \\
gen(\_) & = & \{\text{some dummy value}\} \\
\text{where} & & \\
rep(syntax, n) & = & \{[gen(syntax)\ldots]\} \\
& & \text{(Generate $n$ repetitions of $syntax$)} \\
\end{array}
$$

Nested repetitions incur many more variations as we wish to be sure to capture all possible error interactions. For example, two consecutive '$*$' repetitions produce $3 * 3 = 9$ variations, while two nested '$*$' repetitions produce $1 + 3 + 3*3 = 12$ variations.

\chapter{Evaluation} \label{sec:evaluation}

The evaluation of this thesis is largely empirical and based on case studies facilitated by a proof of concept implementation of the algorithms described in the previous section, along with a small interpreter for a core language. An overview of this implementation is given in Section~\ref{sec:implementation}.

The actual evaluation consists of case studies of expressibility (Section~\ref{sec:case-studies}), reuse of language feature from one language in another (Section~\ref{sec:language-composition}), a brief examination of the correctness of the implementations in the case studies (Section~\ref{sec:correctness-and-performance}), a few performance metrics of the implementation (Section~\ref{sec:performance}), and finally an examination of the errors presented to a user when something goes wrong (Section~\ref{sec:error-reporting}).

For evaluation two programming languages have been implemented: a subset of Lua and a subset of OCaml. The former evaluates the expressibility of a relatively standard imperative language, while the latter evaluates the expressibility of  a relatively standard functional language, with an extra focus on pattern matching.

For each of the languages a few interesting features are discussed, including their implementation, along with several weaknesses of the system.

\section{Implementation} \label{sec:implementation}

This section contains a high level overview of the proof of concept implementation of the ideas in this thesis.

The implementation is written in Haskell and is available at (TODO: refer to public repository, with a specific tag or commit). It is structured similarly to a staged compiler. The stages are as follows:

\begin{description}
  \item[Lexer] The lexer is intended to be the simplest possible lexer that still produces results suitable for the grammars later generated by the syntax constructions. It lexes everything as identifiers, symbols, or string, integer or float literals. Most adjacent punctuation and special characters are merged into single symbols, with a few exceptions to create behavior similar to other programming languages. For example, parentheses will never merge, thus \mintinline{c}{(a++)} will lex as \mintinline{syncon}{symbol('(')}, \mintinline{syncon}{identifier('a')}, \mintinline{syncon}{symbol('++')} and \mintinline{syncon}{symbol(')')}.

  \item[Syntax construction parser] The syntax construction parser is a simple context free grammar, parsed using an Earley parser. It is somewhat complicated by the need to parse arbitrary source code in the expansion description. This is solved by merging the syntax construction grammar with the generated grammar from the source code parser. Due to a limitation in the parsing algorithm used the grammar cannot be modified during parsing, thus the expansion description of a syntax construction cannot use syntax constructions defined in the same file, only ones defined in an earlier file.

  \item[Syntax construction name resolver] The syntax construction name resolver will prepare the parsed syntax constructions for use later in the implementation. All names used are resolved to locations in the syntax description and an expansion function is produced for syntax constructions that are not marked as builtin. Note that this phase does some checking of the syntax construction, but only what is necessary to produce the binding information and expansion function. The checking here is not sufficient to ensure that no binding errors appear during expansion.

  \item[Expansion checker] The expansion checker checks that an expansion can never introduce an error during expansion. Similar to type checking and various other forms of static analysis the expansion checker is not a necessary part, but its absence does of course remove any guarantees on errors.

  \item[Source code parser] The source code parser generates a context free grammar from a set of syntax constructions and then uses it to parse source code using an Earley parser.
  \begin{description}
    \item[Ambiguity detection] If the grammar is ambiguous, and the source code being parsed exposes this ambiguity, one parse tree will be produced for each possible parse, and all of them will be compared to determine the ambiguous sections of the source code.
  \end{description}

  \item[Source code name resolver] The source code name resolver takes a syntax tree generated by the parser and checks for name binding errors. If none are found all symbols are replaced with newly generated symbols in such a way that each symbol appears in a binding position exactly once, while preserving the referencing relation between identifiers.

  \item[Source code expander] The source code expander repeatedly finds a single syntax construction to expand, uses its expansion function, then runs the name resolver over the result, until all remaining syntax constructions are marked as builtin. This is the simplest implementation that will always be correct, but it almost always does far more work than necessary. See Section~\ref{sec:performance} for more information.

  \item[Core language interpreter] The core language interpreter evaluates a syntax tree composed only of syntax constructions that are part of the core language described in Section~\ref{sec:core-language}, but is largely unrelated to syntax constructions as such. It is only used for validation in Chapter~\ref{sec:evaluation}.
\end{description}

\subsection{Core Language} \label{sec:core-language}

Syntax constructions does not specify a core language, but instead treats any construction marked as ''builtin'' as a core language construct, expanding all constructions encountered until only builtin constructions remain. However, to evaluate the expressibility of semantics for syntax constructions such a language must exist. This section describes the core language used throughout the evaluation section.

Both languages are implemented using the same underlying language, i.e. the same set of syntax constructions marked as builtin (for more information, see Section~\ref{sec:design-implementation}). The core language is a largely functional language using curried functions and eager execution, with a few additions:
\begin{itemize}
  \item Sequential composition.
  \item Non-nested bindings, i.e \mintinline{syncon}{defAfter} and \mintinline{syncon}{defAround}. % TODO: probably refer to a previous definition / explanation of this
  \item Builtin, language provided values using special syntax, such as \mintinline{syncon}{#unit}, \mintinline{syncon}{#true} and \mintinline{syncon}{#plus}. Values that provide more complex features include:
  \begin{description}
    \item[Conditional execution] via \mintinline{syncon}{#if}.
    \item[Fixpoint operator] via \mintinline{syncon}{#fix}.
    \item[Mutable references] via \mintinline{syncon}{#ref}, \mintinline{syncon}{#deref}, and \mintinline{syncon}{#assign}.
    \item[Lists] via \mintinline{syncon}{#cons}, \mintinline{syncon}{#head}, \mintinline{syncon}{#tail}, and \mintinline{syncon}{#nil}.
    \item[Continuations] via \mintinline{syncon}{#callcc}.
  \end{description}
  \item Expression functions. Syntactically similar to a function (\mintinline{syncon}{efun x. body} vs. \mintinline{syncon}{fun x. body}) these work around a limitation in syntax constructions. They are described in more detail in the next section, but in brief: they are functions that must be applied immediately and do not introduce a scope. They are used to work around the atomic nature of sub-constructions.
\end{itemize}

\subsubsection{Expression Functions} \label{sec:efun-drawbacks}

Several syntax constructions in the evaluated languages turn out to require some information about their context, be it what value to pattern match on or where to divert control flow to. Some of these cases can be solved by having the syntax construction produce a function and letting its context provide the necessary information via a function argument, but not all.

Problems arise when the syntax construction both requires information from its context, and additionally needs to expose bindings via \mintinline{syncon}{#bind before} or \mintinline{syncon}{#bind after}. Functions introduce a scope around their body, preventing any such bindings from being visible.

To solve this issue this thesis adds an additional kind of function that must be applied immediately and does not introduce a scope around its body. They essentially enable syntax constructions to not only be syntax, but also functions from syntax to syntax. All expression function application should be evaluated \emph{before} other evaluation, since they are intended to model functions during the expansion process, which should be complete before evaluation.

There are two problems with this solution. Firstly, expression functions are a very unsound addition to the core language. Consider this expression:

\begin{minted}{syncon}
(efun a. defAfter x = a) x
\end{minted}

The expression is correct, binding-wise, \mintinline{syncon}{x} is bound inside the expression function and is then in scope on the outside. Evaluating the application of the expression functions produces the following expression however:

\begin{minted}{syncon}
defAfter x = x
\end{minted}

This is not correct, \mintinline{syncon}{x} is not in scope in the definition of \mintinline{syncon}{x}, thus this produces an error.

The second problem is inconvenience, many syntax constructions suddenly need to take arguments and pass them on to their children, even though they themselves have no use of them, simply because the information is further away than the direct parent. This additionally has an impact on composing multiple languages. No new syntax constructions can be added in-between the syntax construction that gives the context and the one that uses it without threading the information, but the odds of a syntax construction from a different language being written to thread the same information seem minuscule, thus importing a syntax construction from a different language is unlikely to work if either language uses that kind of contextual information.

Expression functions are thus clearly an undesirable solution, but a better option is left for future work.

\section{Case Studies in Expressiveness} \label{sec:case-studies}

This section evaluates the expressiveness of syntax constructions through a number of cases. The first two implement subsets of two pre-existing programming languages: OCaml and Lua. The former focuses on implementing pattern matching and a fairly standard functional programming language, while the latter focuses on implementing control flow commonly present in imperative programming languages. The final section examines several smaller examples not present in either language that present problems for syntax constructions.


% TODO: probably write about efuns and their drawbacks separately from each language below, seems weird to either duplicate it, or to just have it in one of the language sections and not the other, since they should have no dependenceies on each other

% TODO: own thoughts, to be made into actual evaluation thoughts later
% Control flow altering things are difficult to include without either explicit support for the particular control flow, or names that can be bound across constructions without appearing in the original source code. The return statement must be able to connect to the return point, without anything outside it altering it. Could be done if the construction returned a "function", that takes the return point and produces the actual expanded code, but that is not possible at present.

% - The efun builtin is unsound, but something like it is required to implement some of the things when each sub-construction is atomic
% - fix seems reasonable
% - callcc probably reasonable, but probably not necessary to have the full support, instead basically have it be a block that you can break out of by calling a function. Not entirely sure if that can be represented in a sound way with the current system
% - ocaml _ pattern and list literals are either inescapably ambiguous, or require rewritings that require knowledge of grammars (which we're trying to avoid)
% - funccall in lua requires splitting out the argument list, to ensure correct recursion (the call must have high precedence, but the argument list must reach for top level expressions)
% - binding expressibility has thus far been sufficient, nothing I've wanted to express has been impossible.
% - defAround allows for recursive definitions, which will be incorrect for most/many languages, but there is nothing that preserves that abstraction

% - (current) absence of checking probably means that at least one implementation is incorrect, only working because the implemented languages are sane when they introduce names. I suspect the general case, when any syntax type can bind in any way, is broken.

\subsection{A Functional Language - OCaml Subset} \label{sec:functional-eval}

% TODO: ref ocaml
The functional language implemented here is a subset of OCaml, with a focus on supporting pattern matching. This section will not examine the entirety of the language definition, most syntax constructions used are fairly simple and rote, focus will instead be placed on the implementation of patterns and pattern matching, as well as additional difficult to express particulars of the language. The full language description can be found in Appendix~\ref{sec:appendix-ocaml}.

\subsubsection{Patterns and Pattern Matching} \label{sec:functional-pattern-matching}

Pattern matching is a feature found in most functional languages, allowing the programmer to simultaneously check the structure of data and extract internal data from it. Patterns describe the expected shape of the data, as well as what internal data should be extracted and bound to names for later use. They can be arbitrarily nested, allowing a simple way of expressing a potentially quite tedious checking process. Listing~\ref{lst:ocaml-match-example} shows an example of a match expression in OCaml, wherein the first match arm with a matching pattern is selected for execution.

\begin{listing}[ht]
\begin{minted}{ocaml}
let result = match [[4]] with
  | [[a], b] ->
    print_string "a:int, b:int list, both bound"
  | _ :: _ :: _ ->
    print_string "nothing bound, length >= 2"
  | [[4]] ->
    print_string "nothing bound"
  | _ ->
    print_string "nothing else matched"
\end{minted}
\caption{Example match expression in OCaml}
\label{lst:ocaml-match-example}
\end{listing}

The above suggests a fairly clear formulation of the syntax and bindings of the match expression using syntax constructions.

\begin{minted}{syncon}
syntax match:Expression =
  "match" e:Expression "with"
  arm:("|" p:Pattern "->" body:Expression)+
{
  #scope arm:(p body)
  #scope (e)
  <...>
}
\end{minted}

Patterns expose their bound names using \mintinline{syncon}{#bind x after} and each match arm introduces a new scope containing its pattern and expression. The patterns themselves are also simple to express in terms of syntax and bindings:

\begin{minted}{syncon}
syntax wildcardPattern:Pattern = "_" {
  <...>
}

syntax bindPattern:Pattern = id:Identifier {
  #bind id after
  <...>
}

syntax integerLiteralPattern:Pattern = i:Integer {
  <...>
}

syntax consPattern:Pattern =
  head:Pattern "::" tail:Pattern
{
  #assoc right
  #prec 10
  <...>
}
\end{minted}

% TODO: this needs to refer to some more thorough description / discussion of the lexer and its choices

Implementing each of these is where problems arise; each pattern must do two things:

\begin{itemize}
  \item Check a value provided by the enclosing pattern or match expression. The result of this check must be usable to change control flow.
  \item Possibly expose a bound name, where the bound value is derived from the checked value, but only needs to be valid if the check succeeded.
\end{itemize}

The former implies a function, each pattern might be implemented as a function receiving the value to check and producing a boolean value signifying the result of the check. The latter precludes the use of a function, since a function introduces a new scope, thus preventing any bound name from being available outside its body.

Other pattern match systems implemented as macros (e.g. \cite{Tobin-Hochstadt2011Extensible-Patt}) tend to have a more complicated match macro, which constructs the final checking code and binding code by examining the actual contents and / or results of the patterns. Syntax constructions on the other hand have no such functionality, child nodes may only be treated atomically, i.e. moved, removed, or duplicated. As such the composing mechanism must be present in the pattern implementation, but it cannot be a function, since it would hide name bindings.

The implementation in this thesis thus uses functions that do not introduce a new scope, the expression functions introduced and discussed in Section~\ref{sec:core-language}. They are used to model functions at a syntax level, they take syntax and produce syntax.

A pattern then becomes essentially a function taking a function to call if the match fails and a value to match against. Implementation becomes as follows:

\begin{minted}{syncon}
syntax wildcardPattern:Pattern = "_" {
  BExpression` efun fail. efun value.
    #unit
}

syntax bindPattern:Pattern = id:Identifier {
  #bind id after
  BExpression` efun fail. efun value.
    defAfter `id(id) = value
}

syntax integerLiteralPattern:Pattern = i:Integer {
  BExpression` efun fail. efun value.
    #if (#equal value `int(i))
      (fun _. #unit)
      (fun _. fail #unit)
}

syntax consPattern:Pattern =
  head:Pattern "::" tail:Pattern
{
  #assoc right
  #prec 10
  BExpression` efun fail. efun value.
    (#if (#equal value #nil)
       (fun _. fail #unit)
       (fun _. #unit);
     defAfter headValue = #head value;
     `t(head) fail headValue;
     defAfter tailValue = #tail value;
     `t(tail) fail tailValue)
}
\end{minted}

The pattern implementing matching against a list literal (i.e. \mintinline{ocaml}{[a, b]} as opposed to \mintinline{ocaml}{a :: b :: []}) is a straightforward extension of the cons pattern.

Continuing, the implementation of \mintinline{ocaml}{match}, while not trivial, is a relatively straightforward composition of its match arms. For each arm, give the pattern the value to check, and a function to call to skip to checking the next arm in case the pattern fails, then evaluate the body and return the result. The implementation uses \mintinline{syncon}{callcc} to make an abortable function: \mintinline{syncon}{#callcc (fun abort. body)} evaluates to one of two things: the argument provided to \mintinline{syncon}{abort}, or the result of evaluating \mintinline{syncon}{body} if \mintinline{syncon}{abort} is never called.

\begin{minted}{syncon}
syntax match:Expression =
  "match" e:Expression "with"
  arm:("|" p:Pattern "->" body:Expression)+
{
  #scope arm:(p body)
  #scope (e)
  #prec 1
  BExpression` #callcc (fun end. (fun val.
    `t(foldr arm prev
         (BExpression`
           #callcc (fun next.
             (`t(p) next val;
              end `t(body)));
           `t(prev))
       (BExpression` #crash)))
    `t(e))
}
\end{minted}

Note that the implementation is somewhat naive, for example no work is shared between the arms, even if there is overlap in the patterns, and there are no warnings for overlapping patterns or incomplete coverage.

% TODO: not neutral, not objective
The inability to express patterns in syntax constructions without the presence of an unsound construction in the core language (\mintinline{syncon}{efun}) signals a clear flaw in the method. The syntax descriptions and binding specifications on the other hand are clear and concise and may be considered a success.

\subsubsection{Patterns in Function Declarations}
\label{sec:ocaml-function-argument-patterns}

Patterns are however not limited only to the \mintinline{ocaml}{match} expression in OCaml, they can additionally be used in function definitions. The most common way of writing such a function uses \mintinline{ocaml}{let} with some syntax sugar to allow writing all the arguments next to the function name, instead of as part of nested \mintinline{ocaml}{fun} expressions. The two definitions below are equivalent, and we would like our OCaml subset to support both.

\begin{minted}{ocaml}
let add a b = a + b
let add = fun a -> fun b -> a + b
\end{minted}

As an informal description of this feature, the identifier being bound in a \mintinline{ocaml}{let} expression can be followed by any number of parameters (patterns), in which case the value being bound should be a curried function with those parameters. The syntax construction \mintinline{syncon}{topLet} below expresses this.

\begin{minted}{syncon}
syntax topLet:Top =
  "let" x:Identifier args:Pattern*
  "=" e:Expression (";" ";")?
{
  #bind x after
  #scope (args (e))
  BExpression` defAfter `id(x) = `t(
    foldr args next
      (BExpression` fun x.
        (`t(args) (fun _. #crash) x;
         `t(next)))
    e)
}
\end{minted}

Using \mintinline{syncon}{topLet} to expand the previous definition of \mintinline{ocaml}{add} we obtain a syntax tree essentially equivalent to the following:

\begin{minted}{syncon}
defAfter add =
  (fun x. <a> (fun _. #crash) x;
    (fun x. <b> (fun _. #crash) x;
      #plus a b))
\end{minted}

where \mintinline{syncon}{<a>} and \mintinline{syncon}{<b>} are placeholders for the expansions of the patterns used for the two arguments. In this particular case both will be \mintinline{syncon}{efun}s ignoring their first argument and using \mintinline{syncon}{defAfter} to expose their second argument. This works, and will work for all sensible patterns we might define for OCaml, but there is a problem. Consider the following two patterns (their expansion is unimportant for the current discussion and thus elided):

\begin{minted}{syncon}
syntax equalPattern:Pattern =
  "=" "(" e:Expression ")"
{
  #scope (e)
  <...>
}

syntax aroundPattern:Pattern =
  id:Identifier
{
  #bind id before
  #bind id after
  <...>
}
\end{minted}

\mintinline{syncon}{equalPattern} evaluates an expression and compares for equality, while \mintinline{syncon}{aroundPattern} is the same as \mintinline{syncon}{bindPattern} except it binds before as well as after. \mintinline{syncon}{aroundPattern} is the real problem, but we need \mintinline{syncon}{equalPattern} to expose it. Conceptually \mintinline{ocaml}{foo} below returns \mintinline{ocaml}{true} if its arguments are equal and produces an error (failed pattern match) otherwise.

\begin{minted}{ocaml}
let foo =(a) a = true
\end{minted}

This is not what happens however, instead an error is introduced during expansion. The expansion below once again uses \mintinline{syncon}{<a>} and \mintinline{syncon}{<b>} as placeholders for the expansions of \mintinline{syncon}{=(a)} and \mintinline{syncon}{a} respectively. We know that \mintinline{syncon}{<a>} refers to \mintinline{syncon}{a}, and that \mintinline{syncon}{<b>} binds this \mintinline{syncon}{a} using \mintinline{syncon}{#bind a before} and \mintinline{syncon}{#bind a after}. But \mintinline{syncon}{<a>} below cannot refer to anything defined by \mintinline{syncon}{<b>}; the latter is enclosed in a function, which introduces a scope and limits the availability of bound names.

\begin{minted}{syncon}
defAfter foo =
  (fun x. <a> (fun _. #crash) x;
    (fun x. <b> (fun _. #crash) x;
      undefined))
\end{minted}

Expansion checking detects this and rejects the expansion specification, preventing an error from being introduced after expansion has started. The problem is that all patterns in OCaml only bind names after themselves, thus this expansion is fine for all cases we are interested in, but since anyone could add a new pattern that works differently we must handle such cases as well.

There are two plausible ways around this, neither of which is supported by syntax constructions at present.
\begin{itemize}
  \item Make a more flexible scope declaration. If \mintinline{syncon}{topLet} had a scope declaration along the lines of \mintinline{syncon}{(args1 (args2 (args3(... (e)))))}, i.e. a new scope is introduced after each argument, disallowing previous patterns from referencing names bound in later patterns, properly reflecting the semantics of the expansion.

  \item Limit syntax constructions of syntax type \mintinline{syncon}{Pattern} to only allow binding after, capturing our intuition of how patterns in OCaml are supposed to work in the actual patterns rather than in syntax constructions that use them.
\end{itemize}

\subsubsection{Lists and Ambiguous Syntax} \label{sec:ambiguous-lists}

A list literal in OCaml is a list of semi-colon separated expressions, enclosed in square brackets. The syntax description of a corresponding syntax construction is straight-forward and can be seen below:

\begin{minted}{syncon}
syntax listLiteral:Expression =
  "[" (e:Expression (";" es:Expression)*)? "]"
{
  <...>
}
\end{minted}

However, OCaml additionally supports sequential composition of expressions in the form of an operator, semi-colon. This syntax construction is also straight forward:

\begin{minted}{syncon}
syntax sequentialComposition:Expression =
  e1:Expression ";" e2:Expression
{
  #assoc right
  #prec 2
  <...>
}
\end{minted}

The resulting composed syntax is ambiguous however: a list literal of length greater than one will never be parsed unambiguously, there is nothing to distinguish the item separators from the sequential composition operator. The OCaml parser special cases this, parsing \mintinline{ocaml}{[1;2;3]} as a list of three elements and \mintinline{ocaml}{[(1;2;3)]} as a list of one element.

Syntax constructions can implement this, but not in a convenient fashion. A first, almost convenient, approach would be to create a new syntax type, \mintinline{syncon}{Statement}, moving sequential composition to it and creating a new syntax construction consisting of a single \mintinline{syncon}{Expression}. This allows us to state when we want an expression including sequential composition (\mintinline{syncon}{Statement}) or excluding sequential composition (\mintinline{syncon}{Expression}). Most previous uses of \mintinline{syncon}{Expression} would now instead be \mintinline{syncon}{Statement}, except list literals.

The first issue with this solution is partially that it is a workaround that requires changes in many places that very much requires each syntax construction to be considered in its context (thus breaking a design goal). The second issue is that it does not work: the original \mintinline{syncon}{Expression} contains syntax constructions with both lower and higher precedence than sequential composition. The rewrite above essentially gives sequential composition lower precedence than all constructions in \mintinline{syncon}{Expression}, thus altering how the language is parsed.

A proper solution might instead require three syntax types: \mintinline{syncon}{ExpressionWith}, \mintinline{syncon}{ExpressionWithout}, and \mintinline{syncon}{ExpressionBase}. The last contains the syntax constructions that were in \mintinline{syncon}{Expression} with higher precedence than sequential composition, while the first two contain duplicates of the remaining syntax constructions, except for sequential composition, which is only present in \mintinline{syncon}{ExpressionWith}.

This solution seems inconvenient enough to not really consider a solution, instead seeing at as a sign of a weakness with syntax constructions as presented in this thesis.

\subsection{An Imperative Language - Lua Subset} \label{sec:imperative-eval}

The imperative language implemented to test the expressiveness of syntax constructions is a subset of Lua. % TODO: ref lua
The purpose is to evaluate expressibility of common control flow constructs present in imperative languages, as such tables and global variables, arguably the more unique aspects of the language, are not implemented. The control flow constructs here implemented are loops, \mintinline{syncon}{break} and \mintinline{syncon}{return}.

\subsubsection{Loops and Break} \label{sec:lua-loops-and-break}

A loop in an imperative language will evaluate its body for side effects, possibly multiple times, for as long as some condition holds. In addition certain statements exist to alter the normal control flow of a loop; in the case of Lua this is limited to the \mintinline{lua}{break} statement, but other languages may contain statements such as \mintinline{java}{continue}, \mintinline{perl}{redo}, or \mintinline{perl}{next}.

This section details the implementation of a \mintinline{lua}{while} loop and the \mintinline{lua}{break} statement.

Starting with the former, \mintinline{lua}{while}, the implementation is relatively straightforward. The condition check and body is expanded into a function that recursively calls itself to perform another condition check and iteration. The recursion is performed using the fixpoint operator \mintinline{syncon}{#fix}.

\begin{minted}{syncon}
syntax while:Statement =
  "while" cond:Expression "do"
  body:Block "end"
{
  BExpression` #fix (fun repeat. fun _.
    #if (#deref `t(cond))
      (fun _.
        (`t(body);
         repeat #unit))
      (fun _. #unit))
}
\end{minted}

Note that the body has syntax type \mintinline{syncon}{Block} as opposed to the more intuitive \mintinline{syncon}{Statement+}. This is due to a peculiarity of the Lua syntax, a block is a list of statements optionally terminated by a break or return statement, which is also the only place where a break or return statement is legal. Since the block structure appears in multiple constructs beside \mintinline{lua}{while} loops, e.g. \mintinline{lua}{function}s and \mintinline{lua}{if} statements, this formulation reduces code duplication.

The definition of \mintinline{syncon}{Block} is as follows:

\begin{minted}{syncon}
syntax type Block = Statement

syntax blockContent:Block =
  (stmnt:Statement ";"?)* ret:Terminator?
{
  #scope (stmnts ret)
  foldr stmnt next
    (BExpression` `t(stmnt); `t(next))
  foldr ret _
    (BExpression` `t(ret))
  (BExpression` #unit)
}
\end{minted}

Continuing with the implementation of the \mintinline{lua}{break} statement we run into some issues. \mintinline{lua}{break} should transfer control flow past the end of the enclosing loop, regardless of the form of said loop, e.g. it should not matter if it is a \mintinline{lua}{while} or \mintinline{lua}{for} loop. This can be done by making all the loops construct an abort function using \mintinline{syncon}{#callcc}, but then this function has to be provided to the \mintinline{lua}{break} statement somehow.

One solution is to have \mintinline{lua}{break} expand to a function taking the abort function as an argument. However, \mintinline{lua}{break} may appear inside some nested statement of the loop body, thus each statement of the body must also be supplied this function, in case it contains a \mintinline{lua}{break}. The code below shows this implementation, threading the abort function through all the statements:

\begin{minted}{syncon}
syntax while:Statement =
  "while" cond:Expression "do"
  body:Block "end"
{
  BExpression` fun _. #callcc (fun break.
    #fix (fun repeat. fun _.
      #if (#deref `t(cond))
        (fun _.
          (`t(body) break;
           repeat #unit))
        (fun _. #unit)))
}

syntax type Block = Statement

syntax blockContent:Block =
  (stmnt:Statement ";"?)* ret:Terminator?
{
  #scope (stmnts ret)
  BExpression` fun break.
  `t(foldr stmnt next
       (BExpression` `t(stmnt) break; `t(next))
     foldr ret _
       (BExpression` `t(ret) break)
     (BExpression` #unit))
}

syntax type Terminator = Statement

syntax break:Terminator = "break" {
  BExpression` fun break. break #unit
}
\end{minted}

This implementation causes a new problem however: \mintinline{lua}{local}, which introduces a new local binding, must also accept the same argument since it is a statement, but a function introduces a scope, preventing the binding from being exposed:

\begin{minted}{syncon}
syntax local:Statement =
  "local" x:Identifier ("=" e:Expression)?
{
  #bind x after
  BExpression` fun _.
    defAfter `id(x) = #ref
      (#deref `t(foldr e _ (e) (BExpression` #unit)))
}
\end{minted}

\mintinline{lua}{break} must be enclosed in a function to receive the abort function, but \mintinline{lua}{local} must not be enclosed in a function, otherwise it cannot expose its binding. A syntax construction has a very limited set of actions it can perform in regards to its child nodes during expansion (moving, removing and duplicating) and thus cannot distinguish between a statement that needs the abort function (e.g. \mintinline{lua}{break} and \mintinline{lua}{if}) and a statement that cannot be wrapped in a function (e.g. \mintinline{lua}{local} and \mintinline{lua}{function}) lest its binding is hidden.

The actual implementation instead uses functions that do not introduce a new scope, the expression functions introduced and discussed in Section~\ref{sec:core-language}. Replacing the outermost enclosing \mintinline{syncon}{fun} in each of the syntax constructions above with \mintinline{syncon}{efun} resolves the error and produces a correct implementation.

\subsubsection{Functions and Return}

A return statement in an imperative language returns control flow to the caller of a function, optionally providing a return value. Reaching the end of a function without encountering a return statement also returns control flow and is equivalent to a return statement without a return value. Listing~\ref{lst:lua-function-example} demonstrates a simple function with a single return.

% TODO: possibly implement "a, b = b, a" assignments, since they are rather more idiomatic
\begin{listing}
\begin{minted}{lua}
function fibonacci(n)
  local prev = 0
  local curr = 1
  local temp
  for i = 1, n-1 do
    temp = b
    b = a + b
    a = temp
  end
  return curr
end
\end{minted}
\caption{An example in Lua demonstrating a simple function}
\label{lst:lua-function-example}
\end{listing}

In the case of nested functions a return statement should only return from the inner-most function. The syntax of a return statement is trivial to describe and can be seen below.

\begin{minted}{syncon}
syntax return:Statement = "return" value:Expression? {
  <...>
}
\end{minted}

The implementation is rather less obvious. The return statement must transfer control flow to a point external to itself, namely to the caller of the enclosing function. However, the situation is rather similar to the \mintinline{lua}{break} statement (see Section~\ref{sec:lua-loops-and-break}), so we employ a similar solution: a function declaration introduces an abort function via \mintinline{syncon}{#callcc} and threads it to all statements via expression functions.

\begin{minted}{syncon}
syntax function:Statement =
  "function" f:Identifier
  "(" (a:Identifier ("," as:Identifier)*)? ")"
  body:Block "end"
{
  #bind f before
  #bind f after
  #bind f, a, as in body
  BExpression` efun _. efun _.
    (defAround `id(f) = #fix (fun recur.
      (defAfter `id(f) = recur;
       `t(foldr a next
            (BExpression` fun `id(a). `t(next))
          foldr as next
            (BExpression` fun `id(as). `t(next))
          (BExpression` fun _. #callcc (fun return.
            `t(body) return (fun _. #crash)))))))
}

syntax return:Return = "return" e:Expression? {
  BExpression` efun return. efun break. `t(
    foldr e _
      (BExpression` return (#deref `t(e)))
    (BExpression` return #unit))
}
\end{minted}

Note that this change adds yet another wrapping \mintinline{syncon}{efun} around all statements, whether it should affect them directly or not.
% TODO: this doesn't feel like a conclusion of the thing

\subsubsection{Function Calls and Precedence} \label{sec:lua-func-call-precedence}

A function call in Lua appears syntactically as an expression evaluating to a function followed by an argument list within parenthesis, while the core language only has single argument functions and uses juxtaposition as application. The translation between the two is straightforward and is as follows:

\begin{minted}{syncon}
syntax funcCall:Expression =
  f:Expression
  "(" (e:Expression ("," es:Expression)*)? ")"
{
  BExpression` `t(
    foldl es prev
      (BExpression` (#deref `t(prev)) `t(es))
    foldl e prev
      (BExpression` (#deref `t(prev)) `t(e))
    f)
    #unit
}
\end{minted}

The final \mintinline{syncon}{#unit} being applied is to allow calling a function that takes no arguments. Compare with the definition of \mintinline{lua}{function} in the previous section. % TODO: reference without an explicit label reference

This definition produces an ambiguous grammar, e.g., \mintinline{lua}{1 + f()} has two allowable parses: \mintinline{lua}{(1 + f)()} and \mintinline{lua}{1 + (f())}. The solution takes the form of precedence, give a function call higher precedence than arithmetic. This solves the ambiguity, but presents a new problem: \mintinline{lua}{f(1 + 2)} is no longer parsed correctly.

The issue stems from how precisely precedence affects allowable parses. Whenever a syntax construction recursively uses its own syntax type (i.e. \mintinline{syncon}{funcCall}, being an \mintinline{syncon}{Expression}, using \mintinline{syncon}{Expression} in the syntax description) those recursive uses may only contain syntax constructions of the same, higher, or undefined precedence. Since addition has lower precedence it is disallowed both in the function being called and in the argument list. The latter is undesired.

This can be solved by an extra layer of indirection. If the argument list is broken out into a separate syntax construction with a different syntax type it's uses of \mintinline{syncon}{Expression} will be entirely unconstrained.

The correct solution then becomes as follows:

\begin{minted}{syncon}
syntax funcCall:Expression =
  f:Expression "(" args:ArgList ")"
{
  #prec 15
  BExpression` `t(args) `t(f) #unit
}

syntax argList:ArgList =
  (e:Expression ("," es:Expression)*)?
{
  BExpression` fun f. `t(
    foldl es prev
      (BExpression` (#deref `t(prev)) `t(es))
    foldl e prev
      (BExpression` (#deref `t(prev)) `t(e))
    (BExpression` f))
}
\end{minted}

This seems clearly undesirable, assigning precedence affects more things than intended and rectifying the problem requires introducing a new syntax type that we most likely would not have wanted otherwise.

\subsection{Smaller Self-contained Examples}

\subsubsection{Variables in Different Domains}

% TODO: domains is probably not the right word here?

Many programming languages have bindings that are split in different domains, the most common example probably being a split between values and types. For example, Listing~\ref{lst:haskell-data-constructor-not-in-scope} shows the result of attempting to use an identifier representing a type in a value position: an unbound error. This despite the same identifier being used on the very next line, thus clearly being bound. The definitions end up in different domains where they neither conflict nor interact.

\begin{listing}[h]
\begin{minted}{haskell}
data T = V
err1 = T -- Not in scope: data constructor ‘T’
ok = V :: T

err2 = case V of
  U -> "U" -- Not in scope: data constructor ‘U’
  u -> "u"
\end{minted}
\caption{Identifiers in Haskell are interpreted differently depending on their syntactical position, as well as the characters in their symbols.}
\label{lst:haskell-data-constructor-not-in-scope}
\end{listing}

Additionally the patterns on the last two lines contain unbound identifiers, but the semantics differ based on the casing of the characters in their symbols. The first of the two matches against a data constructor (the domain was selected by the contents of the symbol), which must be bound, thus an error is produced, while the second binds \mintinline{syncon}{u} to the value of whatever is being matched.

Syntax constructions have only a single domain of bindings, and does not allow differentiating identifiers based on the form of their symbols, and can thus not express the above at all.

\subsubsection{Prolog Pattern Matching} \label{sec:prolog-pattern-matching}

Section~\ref{sec:functional-pattern-matching} evaluates the formulation of a pattern matching system using syntax construction. However, the system implemented only needs to support the capabilities of OCaml, but other languages have pattern matching with more features. For example, Prolog allows matching an unbound variable twice in a pattern, with the semantic meaning of requiring the values at both positions to be equal. Listing~\ref{lst:prolog-absolute} shows a simple example.

\begin{listing}[h]
\begin{minted}{prolog}
abs(A, B) :- A < 0, B is -A.
abs(A, A) :- A >= 0.
\end{minted}
\caption{Prolog rule stating the conditions for the second value being the absolute value of the first.}
\label{lst:prolog-absolute}
\end{listing}

An implementation of the syntax and binding semantics of this in syntax constructions might look as follows:

\begin{minted}{syncon}
syntax variableBinding:Pattern = id:Identifier {
  #bind id after
  <...>
}
syntax equalityPattern:Pattern = id:Identifier {
  <...>
}
\end{minted}

The first syntax construction binds the result of the pattern match, similarly to the pattern matching system in Section~\ref{sec:functional-pattern-matching}. The second has no \mintinline{syncon}{#bind id after} statement and thus requires \mintinline{syncon}{id} to be bound by the environment. If we disallow shadowing these syntax constructions are mutually exclusive; if the symbol is already bound we cannot bind it again without a redefinition error and we must choose \mintinline{syncon}{equalityPattern}, if it is unbound we cannot find that it refers to anything and we must choose \mintinline{syncon}{variableBinding}.

If we allow shadowing of bindings from an outer scope both syntax constructions will be valid in some cases, namely when the symbol is bound in an outer scope. In this case we could shadow it and choose \mintinline{syncon}{variableBinding}, or refer to the pre-existing binding and choose \mintinline{syncon}{equalityPattern}.

Syntax constructions allow shadowing of definitions from an outer scope, thus the above would not be guaranteed to be unambiguous. Furthermore, due to the way parsing is implemented the above would double the number of parse trees for every occurrence of an identifier in a pattern, which in a normal Prolog program would result in a very large number of parse trees. In the interest of performance the current implementation therefore requires all disambiguation to be done syntactically, thus this form of pattern matching could not be expressed at all.

% TODO: strange binding order

% This cannot be described using syntax constructions and their binding descriptions
% foo = a + b + c  // Can refer to everything
%   where
%     a = 1        // Cannot refer to b or c
%     b = a*a      // Cannot refer to c
%     c = a + b

\section{Language Composition} \label{sec:language-composition}

Lua does not normally have pattern matching or destructuring, but the implementation of OCaml in Section~\ref{sec:functional-eval} contains such features, so it seems wasteful to not use them. Section~\ref{sec:patterns-in-lua} details adding destructuring to pre-existing constructions that introduce bindings, while Section~\ref{sec:match-in-lua} attempts to add OCaml's \mintinline{ocaml}{match}-expression to Lua.

\subsection{Patterns in Lua} \label{sec:patterns-in-lua}

We start off by adding destructuring to a \mintinline{lua}{local} binding. The \mintinline{lua}{local} syntax construction is originally implemented as follows:

\begin{minted}{syncon}
syntax local:Statement =
  "local" x:Identifier ("=" e:Expression)?
{
  #bind x after
  #scope (e)
  BExpression` efun _. efun _.
    defAfter `id(x) = #ref
      (#deref `t(foldr e _ (e) (BExpression` #unit)))
}
\end{minted}

Importing all the \mintinline{syncon}{Pattern}s from OCaml, replacing \mintinline{syncon}{Identifier} with \mintinline{syncon}{Pattern}, and making the corresponding change in the expansion yields the following implementation:

\begin{minted}{syncon}
syntax local:Statement =
  "local" x:Pattern ("=" e:Expression)?
{
  #scope (e)
  BExpression` efun _. efun _.
    `t(x) (fun _. #crash)
      (#ref (#deref `t(foldr e _ (e) (BExpression` #unit))))
}
\end{minted}

The semantics of the resulting syntax constructions are essentially as expected, patterns can be used to destructure the value and introduce new bindings. However, a \mintinline{lua}{local} binding can be mutated, which is here implemented by introducing a reference to a value, as opposed to purely the value itself. As such, any pattern that assumes a non-reference value will fail. This stems from a mismatch between the underlying core language and Lua; the core language is explicit about what is a reference and what is not, while Lua will frequently produce references and implicitly dereferences them when required. OCaml, which these patterns were written for, is more in line with the core language and performs no implicit dereference.

We can alleviate this mismatch somewhat by allowing the \mintinline{lua}{local} construct to attempt the pattern both with and without dereferencing. This essentially adds auto-dereferencing to the pattern match, but only for non-nested cases. A pattern appearing somewhere within another pattern will not auto-dereference, so to handle this case we introduce a new pattern, \mintinline{syncon}{derefPattern}, to explicitly match against a reference.

\begin{minted}{syncon}
syntax local:Statement =
  "local" x:Pattern ("=" e:Expression)?
{
  #scope (e)
  BExpression` efun _. efun _.
    (fun expr. #callcc (fun done.
      (#callcc (fun fail. `t(x) fail expr);
       `t(x) (fun _. #crash) (#deref expr))))
    (#ref (#deref `t(foldr e _ (e) (BExpression` #unit))))
}
syntax derefPattern:Pattern =
  "ref" p:Pattern
{
  BExpression` efun fail. efun value.
    `t(p) fail (#deref value)
}
\end{minted}

The remaining binding constructions in Lua (e.g. functions and for loops) can be changed in similar ways to support destructuring. Patterns in function arguments exhibit the same issues here as they do in OCaml, see the end of Section~\ref{sec:functional-pattern-matching}, but otherwise work as expected. We have thus reused patterns from OCaml without needing to change them, only requiring changes to the parts of Lua that should use the patterns.

\subsection{Match in Lua} \label{sec:match-in-lua}

% TODO: rewrite intro to this section after split into two sections
Patterns can do more than simple destructuring, OCaml additionally has a \mintinline{ocaml}{match} construct that enables conditional execution based on which pattern matches a given value. Reusing this construct in Lua turns out to be problematic however, for a few reasons:

\begin{itemize}
\item First, OCaml consists largely of expressions while Lua makes a distinction between statements and expressions. As such, \mintinline{ocaml}{match} is an expression. In OCaml it makes sense to have a control flow construct as an expression, but in Lua most computation requires statements, thus we likely want \mintinline{ocaml}{match} to be a statement.

\item Second, we may consider that in a real world scenario the two languages would likely have been developed entirely independent of each other, i.e. the syntax type \mintinline{syncon}{Expression} of OCaml is distinct from the syntax type \mintinline{syncon}{Expression} of Lua.

\item Third, we only wish to bring in \mintinline{ocaml}{match}, not the entirety of OCaml; it should be possible to write Lua in each of the branches, not OCaml.

\item Fourth, the syntax of OCaml differs significantly from Lua. What is natural in one may very well not be in the other.
\end{itemize}

As such we have a few options for how to integrate \mintinline{ocaml}{match}. Broadly speaking, these options can be split in two groups: explicitly exposing the \mintinline{ocaml}{match} from OCaml, and creating a new syntax construction that expands into a \mintinline{ocaml}{match}, without exposing it.

The former approach immediately runs into problems because of the second point above. To write \mintinline{ocaml}{match} as a statement we require a syntax construction of syntax type \mintinline{syncon}{Statement} consisting of a sole OCaml \mintinline{syncon}{Expression}, a form of bridging syntax construction. Additionally, to write a Lua statement inside a \mintinline{ocaml}{match} branch we require a bridge in the opposite direction. The combination of these two syntax constructions produce an ambiguous grammar however; they can nest any number of times when parsing a statement.

In general, this problem will appear any time we wish to include a syntax construction that is recursive, i.e. that uses its own syntax type in the syntax description. % TODO: mention something about how to solve long term

The alternative reuse option is to create a new syntax construction that expands into a \mintinline{ocaml}{match}, i.e. to follow the tower of languages approach:

\begin{minted}{syncon}
syntax luaMatch:Statement =
  "match" e:Expression "with"
  arms:("case" p:Pattern ":" b:Block)+
  "end"
{
  #scope (e)
  #scope arms:(p b)
  BExpression` efun return. efun break.
    (defAfter e = #deref `t(e);
     `t(foldr arms next
          (Expression` match e with
            | `t(p) -> `t(b) return break
            | _ -> `t(next))
          (BExpression` #crash)))
}
\end{minted}

This syntax construction expands into a nested series of \mintinline{ocaml}{match} expressions from OCaml and can be used like so:

\begin{minted}{lua}
match 4 with
  case 1: print(1)
  case 2: print(2)
  case n: print("n")
end
\end{minted}

\section{Correctness} \label{sec:correctness-and-performance}

Syntax constructions provide no semantics or specification of a core language, thus the choice of core language and a means to run programs in it are largely irrelevant to this thesis. However, we wish to know that the languages defined using syntax constructions have the correct semantics. As such, the syntax construction implementation contains a simple interpreter for the eager, largely functional core language briefly described in Section~\ref{sec:core-language}.

A few programs, written in OCaml and Lua respectively, then form a sort of correctness test suite: if a program executes without any errors in the original implementation, and the syntax construction implementation can parse it, then the results should be the same no matter which of the two is used. The former requirement stems largely from the absence of type checking in syntax constructions, and the latter ensures that the program is part of the defined language subset.

The testing is made slightly more complicated by syntax constructions not supporting importing. Program correctness is checked by comparing printed output, but printing functions tend to be defined externally, via some form of library, standard or otherwise. To smooth over this difference the actual programs tested are slightly different for the original implementations and the syntax construction implementations; they each contain a small prelude that define the same printing functions, but using different underlying mechanisms. Aside from this difference the programs tested are identical.

The test programs implement functions for calculating Fibonacci numbers with the quadratic recursive definition as well as the linear version, and two versions of FizzBuzz. The programs use all syntax constructions at least once between them with the exception of list related constructions in OCaml.

\section{Performance} \label{sec:performance}

The performance of a tool has a large impact on the practicality of using it regularly, thus a few words will here be spent on such issues. Because of the various issues apparent and described throughout the evaluation this section will not be particularly thorough however; if a tool is a poor choice for capability reasons its performance matters little.

In practice all the sub-components of the implementation have been fast enough to not incur any noticeable delay for any of the programs tested, with one very notable exception: the expansion phase is slow. Figure~\ref{fig:performance-graph-with-expansion} shows the result of running the tool on programs of varying length. These programs are constructed by duplicating and concatenating the code in Listing~\ref{lst:lua-fragment} an increasing number of times.

% TODO: maybe use something more latex-y?
\begin{figure}[h]
\includegraphics[width=\textwidth]{resources/performance-graph-with-expansion}
\caption{Stage runtime}
\label{fig:performance-graph-with-expansion}
\end{figure}

\begin{listing}[h]
\begin{minted}{lua}
do
  function identity(x)
    return x
  end
end
\end{minted}
\caption{Simple Lua program that can be concatenated with itself without causing name errors.}
\label{lst:lua-fragment}
\end{listing}

The slowness of expansion is not an inherent property of the method, but rather a consequence of the simplest possible implementation. Expansion of a syntax construction assumes that each symbol exists at most once in binding position (see Section~\ref{sec:expansion-checking} for more information), and name resolution uses gensyms and renaming to ensure this to be true. Any expansion may duplicate one of its sub nodes however, which might produce a tree that has the same symbol in binding position multiple times.

Thus we reach the conclusion that we may need to perform name resolution after an expansion, to replace the duplicated binding symbols with distinct symbols. If the bindings use \mintinline{syncon}{#bind x in} then the affected symbols are confined to sub nodes of the expanded syntax construction, but if they use \mintinline{syncon}{#bind x before} or \mintinline{syncon}{#bind x after} no such guarantee exists. The affected symbols may be arbitrarily far away in the abstract syntax tree, as long as they reside in the same scope, or one of its child scopes.

The naive implementation of expansion then expands a single syntax construction, then runs name resolution on the entire abstract syntax tree to ensure the invariant holds, then repeats until no more syntax constructions can be expanded.

Most syntax constructions do not duplicate its sub nodes however, they are generally used at most once, in which case the extra name resolution pass is unnecessary. In fact, none of the syntax constructions used to implement the OCaml and Lua subsets use a sub node more than once, thus the amount of unnecessary work performed is significant.

\begin{figure}[h]
\includegraphics[width=\textwidth]{resources/performance-graph-without-expansion}
\caption{Stage runtime, excluding expansion}
\label{fig:performance-graph-without-expansion}
\end{figure}

As for the remaining parts of the implementation Figure~\ref{fig:performance-graph-without-expansion} shows the runtime of each of the stages except expansion, for easier readability. Parsing requires an algorithm that can parse any context free grammar and produce all syntax trees in case of ambiguities. The current implementation uses an off-the-shelf implementation of the Earley parsing algorithm. (Todo: ref to library and time complexity analysis?). The algorithms used for grammar generation, expansion checking, expansion, and name resolution have not been examined to determine time complexity, but have in practice had negligible impact on the run time of the implementation.

To summarize, the run time of the current implementation is dominated, by several orders of magnitude, by the expansion phase, which in the common case can be greatly optimized.

\section{Error Reporting} \label{sec:error-reporting}

Since one of the design goals is good error messages this section will spend some time examining the error messages produced by the implementation, and to some extent what the messages could be given some more engineering and user experience effort.

Note that the errors examined are only those related to syntax constructions, i.e. ambiguous source code, incorrect expansions, and name binding errors, nothing on type errors or runtime errors.

\subsection{Ambiguous Source Code} \label{sec:errors-ambiguous}

Syntax constructions provide no guarantee that a composed language has an unambiguous grammar. As such a user may write code that cannot be parsed unambiguously. At that point the user must be presented with a helpful error that can assist with solving the problem, without deep knowledge of the internals of the particular language, nor of grammars and parsing in general.

The error message should quickly direct the user to the actually ambiguous part of the code, and ideally also present what to change to select one of the possible interpretations.

\begin{listing}[h]
\begin{minted}{syncon}
syntax binaryAnd:Expression =
  a:Expression "&" b:Expression
{ <...> }
syntax binaryOr:Expression =
  a:Expression "|" b:Expression
{ <...> }
syntax variable:Expression =
  id:Identifier
{ <...> }
syntax parens:Expression =
  "(" e:Expression ")"
{ <...> }
\end{minted}
\begin{minted}{ocaml}
 a  &  b  |  c
(a  &  b) |  c
 a  & (b  |  c)
\end{minted}
\caption{Example of two operators without defined precedence, and the two interpretations}
\label{lst:undefined-precedence-operators}
\end{listing}

Listing~\ref{lst:undefined-precedence-operators} shows an example of two binary operators without defined precedence, requiring the user to explicitly group the operators. In this case the error produced will mark the correct part of the source code as ambiguous and present two different interpretations:

\begin{itemize}
  \item A \mintinline{syncon}{binaryOr} of a \mintinline{syncon}{binaryAnd} and a \mintinline{syncon}{variable}.
  \item A \mintinline{syncon}{binaryAnd} of a \mintinline{syncon}{variable} and a \mintinline{syncon}{binaryOr}.
\end{itemize}

It is worth noting that the current implementation shows the above information, plus the source code location of each mentioned syntax construction instance (both start and end), but the location information is here omitted for brevity and readability.

Ideally the system could detect that \mintinline{syncon}{parens} have no effect beyond grouping, and that it could be used to force one interpretation or the other, thus using it instead of the textual representation above, but that functionality is not present at the moment. As such the implementation will correctly identify the ambiguous section of the code, and provide a clear enough representation of the different ways to interpret the ambiguity, but no suggested changes are presented.

\begin{listing}[h]
\begin{minted}{syncon}
syntax sum:Expression =
  a:Expression "+" b:Expression
{
  #prec  11
  #assoc left
  <...>
}
syntax let:Expression =
  "let" x:Identifier args:Pattern*
  "=" e:Expression "in" body:Expression
{ <...> }
syntax bindingPattern:Pattern =
  id:Identifier
{ <...> }
syntax integerLiteral:Expression =
  i:Integer
{ <...> }
\end{minted}
\begin{minted}{ocaml}
 let x = 1 in  2  +  x
(let x = 1 in  2) +  x
 let x = 1 in (2  +  x)
\end{minted}
\caption{Example of a definition for a let expression leading to an ambiguous syntax}
\label{lst:ambiguous-let-expr}
\end{listing}

Listing~\ref{lst:ambiguous-let-expr} shows a definition of a \mintinline{ocaml}{let} expression used for a while during construction of the OCaml subset in Section~\ref{sec:functional-eval}. The error presented marks the correct area of code, and presents two different interpretations:

\begin{itemize}
  \item A \mintinline{syncon}{sum} of a \mintinline{syncon}{let} and a \mintinline{syncon}{variable}.
  \item A \mintinline{syncon}{let} of a \mintinline{syncon}{bindingPattern}, an \mintinline{syncon}{integerLiteral} and a \mintinline{syncon}{sum}.
\end{itemize}

The latter interpretation is somewhat less obvious than the former, since we rarely think of a \mintinline{ocaml}{let} expression in terms of its nested syntactical elements, but the meaning is clear enough after comparing the interpretation with the offending source code.

It is worth noting here that parentheses could be used for disambiguation, similar to above, but the syntax of OCaml dictates that the code in Listing~\ref{lst:ambiguous-let-expr} should be unambiguously parsed as the second alternative. The solution in this case is to give \mintinline{ocaml}{let} a low precedence.

% TODO: write about [1; 2; 3]
\begin{listing}[h]
\begin{minted}{syncon}
syntax sequentialComposition:Expression =
  a:Expression ";" b:Expression
{
  #assoc right
  #prec 2
  <...>
}
syntax listLiteral:Expression =
  "[" (e:Expression (";" es:Expression)*)? "]"
{ <...> }
syntax variable:Expression =
  v:Identifier
{ <...> }
\end{minted}
\begin{minted}{ocaml}
[ a ; b ; c ]
[(a ; b ; c)]
[(a ; b); c ]
[ a ; b ; c ]
\end{minted}
\caption{Example of an ambiguous list literal. The alternative interpretations are presented as OCaml would parse them.}
\label{lst:ambiguous-list-literal}
\end{listing}

Listing~\ref{lst:ambiguous-list-literal} shows the list literal as originally implemented in the OCaml subset (see Section~\ref{sec:ambiguous-lists} for more details). The ambiguity occurs since the item separator in a list is the same as the sequential composition operator. The interpretations presented by the error message are as follows:

\begin{itemize}
  \item A sequence of a \mintinline{syncon}{sequentialComposition} and a \mintinline{syncon}{*} repetition covering no source code.
  \item A sequence of a \mintinline{syncon}{sequentialComposition} and a \mintinline{syncon}{*} repetition covering \mintinline{syncon}{; c}.
  \item A sequence of a \mintinline{syncon}{variable} and a \mintinline{syncon}{*} repetition covering \mintinline{syncon}{; b ; c}.
\end{itemize}

These interpretations expose rather more implementation details than the previous ones. A \mintinline{syncon}{sequence} refers to a parenthesized list of syntactical elements in a syntax description, i.e. \mintinline{syncon}{(e: Expression (";" es:Expression)*)} above. A \mintinline{syncon}{*} repetition refers to a syntactical element repeated using an EBNF operator, i.e. \mintinline{syncon}{(";" es:Expression)*} above.

The \mintinline{syncon}{sequence} is reported instead of \mintinline{syncon}{listLiteral} since, implementation-wise, it is the closest node in the syntax tree that covers the entire ambiguity. The \mintinline{syncon}{listLiteral} also covers the square brackets, but those are parsed unambiguously. The repetition is reported as such instead of some number of \mintinline{syncon}{variable}s simply because it was the easiest information to obtain given the current implementation.

A language implementer would probably appreciate the above representations of the interpretations, while an end user would most likely rather want the closest enclosing syntax construction and a list of its child syntax constructions, regardless of internal structure of the syntax construction and it possibly covering some unambiguous parts of the code.

Either representation is simple to obtain, and could potentially be selected by the user depending on whether they are an end user or a language implementer.

\subsubsection{Static and Dynamic Ambiguity Detection} \label{sec:static-dynamic-ambiguity}

% TODO: link this to the previous section properly, i.e. transition into a question on how to ensure the correctness of a language implementation syntax-wise, and how that question is somewhat close to 'normal' static vs dynamic type checking and what constitutes correct there
The errors presented above can be likened to runtime errors in a dynamic programming language, i.e. a programming language without compile time type checking: an ambiguous language only produces errors at parse-time and an incorrect program only produces errors at run-time.

Correctness in a dynamically typed language tends to depend more on testing, manual or automated, than static guarantees, since the latter are largely absent. Testing for ambiguity is rather straightforward, merely provide example code and state whether it is ambiguous or not, then try parsing it.

However, ambiguity is generally a property of a set of syntax constructions, not of any syntax construction in itself. Passing tests for one set of syntax constructions (i.e. a language) give little to no information for another set, even if the two intersect (i.e. they share syntax constructions). Thus a complete set of tests need to be created for each language.

This is analogous to integration tests in a dynamically typed language; a new system (i.e. a new composition of sub-systems) require new integration tests. Regular programming languages additionally tend to have unit tests, which compose somewhat better and give some confidence in the correctness of the composition. Thus testing for a dynamically typed programming language is likely to scale better than testing for ambiguity with syntax constructions.

Static ambiguity detection on the other hand, i.e. compile-time checking, would likely scale better, since it would be automatic and ideally complete for each language without extra work for the language designer. This thesis makes no attempt at determining the ambiguity of a language, instead leaving such an approach for future work.

\subsection{Expansion Specification Errors}

The tool implemented for this thesis performs the checking necessary to ensure that no syntax construction has an expansion that may produce a malformed syntax tree. The errors presented when such an expansion is encountered are informative, but no effort has been put into making the presentation user friendly. Keeping that in mind, the errors presented are one of the following:

\begin{description}
  \item[Missing export] The syntax construction specifies \mintinline{syncon}{#bind x before} or \mintinline{syncon}{#bind x after}, but the expansion does not expose \mintinline{syncon}{x}. This error is also produced if a syntax construction appears in the top-level scope in the syntax construction before expansion, but is in an inner scope or not present after expansion, since it could also export a symbol.

  \item[Dependency error] The checker uses a concept of a dependency, which is either an \mintinline{syncon}{Identifier} or a syntax construction along with a direction (before or after). These define the sources from which a reference may find its binding. Since any of these sources may be used the dependency set of a syntax construction after expansion must be a superset of its dependency set before expansion.

  \item[Self dependency] Following the definition of a dependency above, if a syntax construction depends on itself (i.e. on a copy of itself) after expansion, and it binds a symbol, then this symbol is defined (at least) twice. Technically this is only an error if at least two of the definitions are in the same scope, shadowing is allowed as long as it is in a child scope, but the implementation is currently slightly overly cautious and rejects any self dependency.

  \item[Binding error] Since an expansion can use any syntax in its definition it may also introduce new bindings and identifiers. These are checked the same as normal name resolution and reported in a similar fashion.
\end{description}

The errors are presented in terms of hypothetical syntax trees with the syntax construction being checked as the root. References to elements in this tree are by position in the tree, since not all things are named in the syntax description and even things that are named might be duplicated because of an EBNF operator.

As a non-trivial example, consider this implementation of an if statement (from the Lua implementation in Section~\ref{sec:imperative-eval}). The expansion uses the core language builtin \mintinline{syncon}{#if}, which requires the two branches to be single argument functions that are passed \mintinline{syncon}{#unit} if they are chosen for execution.

\begin{minted}{syncon}
syntax if:Statement =
  "if" cond:Expression
  "then" then:Block
  elseif:("elseif" econd:Expression "then" ethen:Block)*
  ("else" else:Block)? "end"
{
  #scope (then)
  #scope elseif:(ethen)
  #scope (else)

  BExpression` efun return. efun break. #if (#deref `t(cond))
    (fun _. `t(then) return break)
    (fun _. `t(
      foldr elseif next
        (BExpression` #if (#deref `t(econd))
          (fun _. `t(ethen) return break)
          (fun _. `t(next)))
      foldr else _
        (BExpression` `t(else) return break)
      (BExpression` #unit)))
}
\end{minted}

Expansion checking produces several errors, here slightly abbreviated and clarified, and further explained and summarized later:
\begin{enumerate}
  \item \mintinline{syncon}{syntax#[1]} needs \mintinline{syncon}{before} exports from \mintinline{syncon}{syntax#[4,0,1]}.
  \item \mintinline{syncon}{syntax#[1]} needs \mintinline{syncon}{before} exports from \mintinline{syncon}{syntax#[4,1,1]}.
  \item \mintinline{syncon}{syntax#[3]} needs \mintinline{syncon}{before} exports from \mintinline{syncon}{syntax#[4,0,1]}.
  \item \mintinline{syncon}{syntax#[3]} needs \mintinline{syncon}{before} exports from \mintinline{syncon}{syntax#[4,1,1]}.
  \item \mintinline{syncon}{syntax#[4,0,1]} needs \mintinline{syncon}{before} exports from \mintinline{syncon}{syntax#[4,1,1]}.
  \item \mintinline{syncon}{syntax#[4,0,3]} needs \mintinline{syncon}{before} exports from \mintinline{syncon}{syntax#[4,1,1]}.
  \item \mintinline{syncon}{syntax#[4,0,1]} \mintinline{syncon}{before} exports should be exported.
  \item \mintinline{syncon}{syntax#[4,0,1]} \mintinline{syncon}{after} exports should be exported.
  \item \mintinline{syncon}{syntax#[4,1,1]} \mintinline{syncon}{before} exports should be exported.
  \item \mintinline{syncon}{syntax#[4,1,1]} \mintinline{syncon}{after} exports should be exported.
\end{enumerate}

The \mintinline{syncon}{syntax#[<...>]} expressions refer to elements in the syntax tree, for example, in \mintinline{syncon}{syntax#[4,0,1]} the \mintinline{syncon}{4} refers to \mintinline{syncon}{elseif:(<...>)*}, \mintinline{syncon}{0} refers to the first repetition of \mintinline{syncon}{elseif:(<...>)} (note the absence of \mintinline{syncon}{*}), and \mintinline{syncon}{1} refers to the second element in the parentheses, namely \mintinline{syncon}{econd}. Similarly, \mintinline{syncon}{syntax#[4,1,1]} also refers to \mintinline{syncon}{econd}, but in a different repetition of \mintinline{syncon}{elseif}.

Using this information we can summarize the errors to the following list:

\begin{enumerate}
  \item \mintinline{syncon}{cond} needs \mintinline{syncon}{before} exports from \mintinline{syncon}{econd}. (1, 2)
  \item \mintinline{syncon}{then} needs \mintinline{syncon}{before} exports from \mintinline{syncon}{econd}. (3, 4)
  \item \mintinline{syncon}{econd} needs \mintinline{syncon}{before} exports from later repetitions of \mintinline{syncon}{econd}. (5)
  \item \mintinline{syncon}{ethen} needs \mintinline{syncon}{before} exports from later repetitions of \mintinline{syncon}{econd}. (6)
  \item Both \mintinline{syncon}{before} and \mintinline{syncon}{after} exports from \mintinline{syncon}{econd} need to be re-exported. (7-10)
\end{enumerate}

The errors arise because the expansion puts repetitions of \mintinline{syncon}{econd} inside functions, later repetitions nested further in, which restricts their exports, hiding them from most preceding elements as well as the surrounding syntax tree.

The fix is two wrap the conditions in their own scope and restrict their exports before expansion; we most likely do not intend for variables bound in the condition to be visible outside of the if-statement.

The errors presented here are perhaps not particularly nice to read or understand, but they contain the information needed to construct better messages, it ''just'' requires some more engineering and user experience effort.

\subsection{Binding Errors}

The errors presented by the implementation in the presence of binding errors add nothing new beyond what is common in most compilers. The following errors are reported:

\begin{description}
  \item[Undefined symbol] If an identifier is encountered in a non-binding position and no binding with the same symbol is in scope this error is reported, along with the source code location of the unresolved identifier.

  \item[Symbol already defined] Bindings may shadow bindings from outer scopes, but if a symbol is defined twice in the same scope, and the bindings cover an overlapping area of code, then this error is reported. The source code location of both definitions are included.

  The overlapping requirement is important: a binding that can be used both before and after its definition is defined by using both \mintinline{syncon}{#bind x before} and \mintinline{syncon}{#bind x after}. This binds the same symbol twice in the same scope, but since the bound areas do not overlap, no error is reported.
\end{description}

% TODO: this sentence is not very clear to most I think
Since all errors are independent in the sense that none of them is the cause of any other, they can all be collected and presented to the user at once.

No binding errors can be encountered during expansion of syntax construction, nor when all syntax constructions are expanded, thus all binding errors are reported in terms of the original source code written by the end user.

\chapter{Limitations, Future Work and Conclusion}

\section{Limitations and Future Work}

\begin{description}
  \item[Contextual Information] Certain programming language features require information specified by their context without a programmer using an explicit reference to that context, for example, a \mintinline{lua}{break} statement must abort the innermost surrounding loop without any reference to it. Other examples of this can be found in sections \ref{sec:imperative-eval} (\mintinline{lua}{break} and \mintinline{lua}{return}) and \ref{sec:functional-eval} (pattern matching). Syntax constructions have no concept of this, it is instead implemented using either regular functions in the core language, or expression functions, introduced in Section~\ref{sec:core-language}. However, as elaborated later in the same section (starting on page \pageref{sec:efun-drawbacks}), these are a very poor, unsound, solution that unnecessarily couple the implementation of different syntax constructions to each other.

  A more sound solution for future work may be to create a form of lexical bindings, where a syntax construction may set the value of such a binding for all its descendants, and a syntax construction that uses such a binding may not be used where it is unset (e.g. \mintinline{lua}{break} may only be used inside a loop, which would set such a binding).

  \item[Ambiguity Control] Syntax constructions allow for ambiguity in the syntax of a programming language where desired, but provides poor control over it. The language designer gets no help in ensuring its presence or absence but must instead test for it themselves. Additionally, when ambiguity is present but undesired the tools to remove it are frequently not very convenient, page~\pageref{sec:lua-func-call-precedence} for example contains an unintentional ambiguity with a slightly inconvenient solution, while page~\pageref{sec:ambiguous-lists} contains an unintentional ambiguity with a significantly inconvenient solution.

  Future work must here come in two different, but related flavors:
  \begin{itemize}
    \item Control of where ambiguity appears. This requires static analysis of the grammar, which in the general case is is undecidable for a context free grammar \cite{Cantor1962On-The-Ambiguit}. The question then becomes if a heuristic is sufficient, or if some more restricted form of grammar can support the desired workflow of defining the syntax of each syntax construction separately.

    % TODO: the suggested future work things here might not be well described, and might be too specific? Should they truly be here in this fashion?
    \item Tools to remove or add ambiguity. Precedence as a number easily turns unwieldy, and turns out to be insufficiently flexible (e.g. see p.~\pageref{sec:ambiguous-lists}). Disallowing specific syntax constructions from appearing in certain positions would solve this particular problem (the expressions in a list literal cannot be sequential composition), and may prove to be a more flexible tool.
  \end{itemize}

  \item[Separated Symbol Domains] All bound symbols are treated the same by syntax constructions, but many languages partition them into groups, a common example being the separation of types from values: names bound as types are fully separated from names bound as values. % TODO: ref to place where this is discussed?

  \item[Disambiguation by binding or reference] Some programming languages have features that behave differently depending on whether an identifier is previously bound or not. Page~\pageref{sec:prolog-pattern-matching} discusses this in more detail, but the most natural way to express this with syntax constructions is to define two syntax constructions, one for when the identifier is bound and one for when it is not, but otherwise syntactically identical, which the system as it stands cannot handle unambiguously.

  \item[Namespaces] Though explicitly ignored in this thesis (see delimitations in Section~\ref{sec:delimitations}), namespaces and the ability to import definitions from them are important parts of many programming languages and should play a part in future work.
\end{description}

\section{Conclusion}

This thesis has presented a first attempt at a method for defining programming languages through individual features, allowing reuse and composition of much smaller parts than entire languages. From a subjective point of view the experience of using syntax constructions to define languages was pleasant when things were working, and the error messages were informative when they did not, but fixing the problems was frequently painful.

Ambiguity control in particular was insufficient, the controls available for the purpose had unintended side-effects (precedence in function calls in Lua, see p.~\pageref{sec:lua-func-call-precedence}) or could not solve the issue at all, requiring large amounts of duplication (lists in OCaml, see p.~\pageref{sec:ambiguous-lists}).

Specifying binding semantics worked well however, with the exception of requiring us to deal with a case we would rather forbid (function argument patterns in OCaml, see p.~\pageref{sec:ocaml-function-argument-patterns}).

Syntax constructions do not live up to the goals initially presented, but they do provide a reasonable first step.

\printbibliography[heading=bibintoc]

\appendix

\chapter{Language Definition: OCaml Subset} \label{sec:appendix-ocaml}

\inputminted{syncon}{implementation/languages/ocaml/language}

\chapter{Language Definition: Lua Subset} \label{sec:appendix-lua}

\inputminted{syncon}{implementation/languages/lua/language}

\end{document}
